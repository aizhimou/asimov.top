[{"categories":["Media"],"content":"本文主要内容翻译自一篇介绍 FFmpeg 入门知识的英文 Web 幻灯片，加入了一些个人理解与学习注释。 我翻译这篇文章的目的，一是学习 FFmpeg 相关的知识，二是练习英文翻译能力。但是受限于个人水平，有些英文专业名词翻译成中文有些怪异，碰到觉得翻译不好的地方，文中会附上原文词语，建议结合原文更好理解。 原文作者是一名视频质量与体验质量研究员，同时也是 Universität Ilmenau 的一名博士生和助理研究员。原文写得非常棒，文本质量和排版都比我的译文更好，建议有英文基础的朋友直接阅读原文，链接：FFMPEG ENCODING AND EDITING COURSE ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:0:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"课程目标 基本概念 安装 ffmpeg 和相关工具 编码视频 使用过滤器（滤镜） 分析视频 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:1:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"前置要求 这份幻灯片（原文是 web 幻灯片的形式） 已安装 ffmpeg、ffprobe、ffplay 一些样例视频，比如：Big Buck Bunny ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:2:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"资源 如果你需要一些样例视频用做测试，可以参考 VQEG (Video Quality Experts Group) 的说明：https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx FFMPEG 简介 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:3:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"关于 用于多媒体编辑、转换… 的免费开源软件 开始于 2000 年 持续维护和开发至今 类似或相关（且有用）的框架： ImageMagick MLT Framework ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:4:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"工具 FFmpeg 是一系列软件包和类库的组合，包括： 命令行工具：ffmpeg、ffprobe、ffplay 类库：libavformat, libavcodec, libavfilter, … 其中类库被许多项目使用（比如 VLC、MLT Framework 等）并且可以在 C/C++ 代码中使用 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:5:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"类库 libavformat：读取和写入容器格式（AVI, MKV, MP4, …） libavcodec：读取和写入编解码器（H.264, H.265, VP9, …） libavfilter：各种个样的视频和音频过滤器 以及其他… 关于如何以编程的方式使用类库的样例： http://leixiaohua1020.github.io/#ffmpeg-development-examples 以及一些其他有用的（非 FFmpeg 的）类库： OpenCV: 面向更多类型的信号处理 Python: MoviePy, pyav, scikit-video ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:6:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"架构 简化的 FFmpeg 整体架构 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:7:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"安装 / 编译 安装方式 优势 劣势 从源码编译 可以自定义所有配置，工具，编解码器 费时间，升级困难 下载预编译版本 简单且快速 不提供所有编解码器，需要手动升级 包管理器安装 简单且快速 可能不是最新版本，也不提供所有编解码器 获取源代码和预编译版本：http://ffmpeg.org/download.html Windows 预编译版本：https://ffmpeg.org/download.html#build-windows ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:8:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"获取帮助 官方文档：https://ffmpeg.org/ffmpeg-all.html Wiki 知识库：http://trac.ffmpeg.org/wiki IRC 聊天室：#ffmpeg 邮件列表：https://lists.ffmpeg.org/mailman/listinfo/ffmpeg-user/ Stack Overflow: https://stackoverflow.com/ and use #ffmpeg Super User: http://superuser.com/ and use #ffmpeg … 或者问我 基础视频编码概念 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:9:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"容器格式（Container Formats） 容器包含媒体数据。比如： MP4: MPEG-4 Part 14 容器，用于 H.264, H.264, AAC 音频, … MKV: 可以包含几乎所有媒体编码格式的万能容器 WebM: MKV 容器的子集，主要用于网络媒体流 AVI: 传统容器 在 FFmpeg 中查看支持的容器： $ ffmpeg -formats File formats: D. = Demuxing supported .E = Muxing supported -- D 3dostr 3DO STR E 3g2 3GP2 (3GPP2 file format) E 3gp 3GP (3GPP file format) D 4xm 4X Technologies E a64 a64 - video for Commodore 64 D aa Audible AA format files ... ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:10:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"编解码器（Codecs） 编解码器 = 编码器 / 解码器 如何编码和解码视频、音频…的规范 通常不是如何编码 / 压缩数据的规范 有时候 “编解码器” 被用于直接指代 编码 / 解码的软件 译者按：Codec 可以理解为编码协议，不同的协议处理数据（音视频）的规则不一样，协议有免费开源的，自然也有商用私有的。 查看 FFmpeg 支持的编解码器： $ ffmpeg -codecs Codecs: D..... = Decoding supported .E.... = Encoding supported ..V... = Video codec ..A... = Audio codec ..S... = Subtitle codec ...I.. = Intra frame-only codec ....L. = Lossy compression .....S = Lossless compression ------- D.VI.. 012v Uncompressed 4:2:2 10-bit D.V.L. 4xm 4X Movie D.VI.S 8bps QuickTime 8BPS video .EVIL. a64_multi Multicolor charset for Commodore 64 (encoders: a64multi ) .EVIL. a64_multi5 Multicolor charset for Commodore 64, extended with 5th color (colram) (encoders: a64multi5 ) D.V..S aasc Autodesk RLE D.VIL. aic Apple Intermediate Codec ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:11:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"最重要的有损编解码器（Lossy Codecs） 目前主要使用的，由 ITU / ISO 标准化： 🎥 H.262 / MPEG-2 Part H: 主要是广播、电视使用，用于向后兼容 🎥 H.264 / MPEG-4 Part 10: 当今视频编码器的事实标准（2018年） 🎥 H.265 / HEVC / MPEG-H: H.264 的继任者，视频质量提升 50% 🔊 MP3 / MPEG-2 Audio Layer III: 曾经是音频编码的事实标准 🔊 AAC / ISO/IEC 14496-3:2009: 高级音频编码标准 免授权费（免版权的）主要竞争者： 🎥 VP8: 由 Google 提出的开源免费编解码器（如今使用并不多） 🎥 VP9: VP8 的继任者，几乎和 H.265 一样好 🎥 AV1: VP9 的继任者，声称比 H.265 更好 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:12:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"最重要的无损编解码器（Lossless Codecs） 无损编解码器在存档、编辑…领域非常有用 无损编码 = 在较小的文件体积下没有压缩损失 🎥 Raw YUV, HuffYUV, FFV1, ffvhuff … 🔊 Raw PCM, FLAC, ALAC, … 当然，还有 “视觉无损” 编解码器： 🎥 Apple ProRes, Avid DNxHD, JPEG2000, high-quality H.264/H.265, … High bitrate and usually only I-frames（高码率帧内压缩） ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:13:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"编码器（Encoders） 编码器是输出编码后字节流的实际软件 编码器会影响视频的质量和性能，一些编码器会好于另一些（有些免费，有些不免费） 译者按：Encoder 可以理解为 Codec（编码协议）的具体实现方式，不同的实现方式各有优劣，有免费开源的实现，也有商用私有的实现。 例如： 🎥 libx264: 最受欢迎的 H.264 编码器 🎥 NVENC: NVIDIA GPU 专用的 H.264 编码器 🎥 libx265: 免费开源的 HEVC（H.265）编码器 🎥 libvpx: Google 推出的 VP8 和 VP9 编码器 🎥 libaom: AV1 编码器 🔊 libfdk-aac: AAC 编码器 🔊 aac: 原生（默认） FFmpeg AAC 编码器 … 总的来说，编码器方面有很多竞争。 查看 FFmpeg 支持的编码器： $ ffmpeg -encoders Encoders: V..... = Video A..... = Audio S..... = Subtitle .F.... = Frame-level multithreading ..S... = Slice-level multithreading ...X.. = Codec is experimental ....B. = Supports draw_horiz_band .....D = Supports direct rendering method 1 ------ V..... a64multi Multicolor charset for Commodore 64 (codec a64_multi) V..... a64multi5 Multicolor charset for Commodore 64, extended with 5th color (colram) (codec a64_multi5) V..... alias_pix Alias/Wavefront PIX image ... ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:14:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"像素格式（Pixel Formats） 视频流中原始像素的表示 指定亮度 / 色彩和饱和度的采样顺序 查看 FFmpeg 支持的像素格式： ➜ ~ ffmpeg -pix_fmts --hide_banner Pixel formats: I.... = Supported Input format for conversion .O... = Supported Output format for conversion ..H.. = Hardware accelerated format ...P. = Paletted format ....B = Bitstream format FLAGS NAME NB_COMPONENTS BITS_PER_PIXEL BIT_DEPTHS ----- IO... yuv420p 3 12 8-8-8 IO... yuyv422 3 16 8-8-8 IO... rgb24 3 24 8-8-8 IO... bgr24 3 24 8-8-8 IO... yuv422p 3 16 8-8-8 IO... yuv444p 3 24 8-8-8 IO... yuv410p 3 9 8-8-8 IO... yuv411p 3 12 8-8-8 ... 译者按：这块我也没看明白，先放着吧 😂，我比较关心视频编码（数据压缩）的部分 用 FFmpeg 命令行工具编码 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:15:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"基本语法 ffmpeg \u003cglobal-options\u003e \u003cinput-options\u003e -i \u003cinput\u003e \u003coutput-options\u003e \u003coutput\u003e 全局参数（global-options）用于日志输出，覆盖文件等 输入参数（input-options）用于读取文件 输出参数（output-options）用于： 转换（编码格式，视频质量等） 过滤器（filtering） 流映射（stream mapping） … 完整的帮助 / 参考文档：ffmpeg -h full 或者 man ffmpeg 但是内容非常非常多！ ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:16:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"转码和转换（TRANSCODING AND TRANSMUXING） 从一种编码格式转码到另一种编码格式（Transcoding），比如用 libx264 编码器转为 H.264 编码格式 ffmpeg -i \u003cinput\u003e -c:v libx264 output.mp4 从一种视频容器转换为另一种视频容器，不用重新编码 ffmpeg -i input.mp4 -c copy output.mkv ffmpeng 会从输入中获取一个视频、音频和字幕流并将其映射到输出文件中。 注释： -c 设置编码器（参考 ffmpeg -encoders) -c 复制比特流（copy only copies bitstream） -c:v 设置视频编码器 -c:a 设置音频编码器 -an 和 -vn 分别是禁用音频和视频流 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:17:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"转码的背后（TRANSCODING BACKGROUND） 节选自 http://ffmpeg.org/ffmpeg-all.html： ffmpeg […] read[s] input files and get packets containing encoded data from them. When there are multiple input files, ffmpeg tries to keep them synchronized […]. Encoded packets are then passed to the decoder. […] The decoder produces uncompressed frames […] which can be processed further by filtering […]. After filtering, the frames are passed to the encoder, which encodes them and outputs encoded packets. Finally those are passed to the muxer, which writes the encoded packets to the output file. 译者按：这段专业名词太多，尝试翻译了几次都感觉太僵硬太奇怪了。其实就是前面那个架构图的文字描述。 大概意思是：ffmpeg 从文件中读取编码过的数据包，如果有多个文件，ffmpeg 会尝试同步多个文件。然后这些编码过的数据包（encoded packets）会被发送到解码器（decoder），解码器将这些数据包解码为原始视频帧（uncompressed frames），然后就可以使用各种过滤器（filters）来处理这些视频帧（frames），然后这些处理过后的视频帧率会被发送到编码器（encoder），编码器会将他们编码为数据包（encoded packets），最后这些数据包会被发送到混流器（muxer），最终混流器将这些数据包输出为文件。 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:17:1","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"查找和剪切（SEEKING AND CUTTING） 从 \u003c起始点（start）\u003e 用 \u003c持续时间（duration）\u003e 或者 \u003c结束点（end）\u003e 来剪切视频 ffmpeg -ss \u003cstart\u003e -i \u003cinput\u003e -t \u003cduration\u003e -c copy \u003coutput\u003e ffmpeg -ss \u003cstart\u003e -i \u003cinput\u003e -to \u003cend\u003e -c copy \u003coutput\u003e 例如： ffmpeg -ss 00:01:50 -i \u003cinput\u003e -t 10.5 -c copy \u003coutput\u003e ffmpeg -ss 2.5 -i \u003cinput\u003e -to 10 -c copy \u003coutput\u003e ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:18:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"使用 SEEKNG 注意事项 重新编码视频时，seeking 总是可以精确到时间戳 当拷贝视频流而没有经过重新编码时（-c copy），ffmpeg 可能会包含未显示但是必要的帧 使用 -c copy 剪切视频时，可能会产生黑帧开头的视频（在某些播放器上） 详情可以查看： http://trac.ffmpeg.org/wiki/Seeking https://superuser.com/questions/138331/using-ffmpeg-to-cut-up-video 译者按：之所以在剪切视频时，转码于不转码存在时间准确性上的差别，是因为转码时，原视频的每一帧都会被 ffmpeg 读取出来（也就是 decode），因此可以准确的定位到时间点，但是如果没有转码，直接复制视频流的时候，ffmpeg 并不会逐帧分析视频，而是采用“关键帧”来定位时间，当设定的时间点上没有关键帧时，就会出现一些时间偏差。 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:18:1","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"设置质量 输出文件的质量取决于编码器默认设置于原材料（也就是原视频）的质量 不要在未设置任何质量水平的情况下进行编码 通常情况下，你需要选择一个目标比特率或者质量水平 目标比特率取决于视频的类型，大小和帧率 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:19:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"设置质量的参数 可能会使用到的参数（仅示例） -b:v 或者 -b:a 设置比特率 比如：-b:v 1000K = 1000kbit/s, -b:v 8M = 8Mbit/s q:v 或者 -q:a 设置固定质量参数 比如：-q:a 2 设置为原生 AAC 编码器 一些编码方式的示例： -crf 用来设置 libx264/libx265 的 恒定码率因子（Constant Rate Factor） vbr 为 FDK-AAC 编码器设置固定质量 还有很多其他参数，可以使用 ffmpeg -h encoder=libx264 for examples 查看，如下 ➜ ~ ffmpeg -h encoder=libx264 for examples -hide_banner Encoder libx264 [libx264 H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10]: General capabilities: dr1 delay threads Threading capabilities: other Supported pixel formats: yuv420p yuvj420p yuv422p yuvj422p yuv444p yuvj444p nv12 nv16 nv21 yuv420p10le yuv422p10le yuv444p10le nv20le gray gray10le libx264 AVOptions: -preset \u003cstring\u003e E..V....... Set the encoding preset (cf. x264 --fullhelp) (default \"medium\") -tune \u003cstring\u003e E..V....... Tune the encoding params (cf. x264 --fullhelp) -profile \u003cstring\u003e E..V....... Set profile restrictions (cf. x264 --fullhelp) -fastfirstpass \u003cboolean\u003e E..V....... Use fast settings when encoding first pass (default true) -level \u003cstring\u003e E..V....... Specify level (as defined by Annex A) -passlogfile \u003cstring\u003e E..V....... Filename for 2 pass stats -wpredp \u003cstring\u003e E..V....... Weighted prediction for P-frames -a53cc \u003cboolean\u003e E..V....... Use A53 Closed Captions (if available) (default true) -x264opts \u003cstring\u003e E..V....... x264 options -crf \u003cfloat\u003e E..V....... Select the quality for constant quality mode (from -1 to FLT_MAX) (default -1) -crf_max \u003cfloat\u003e E..V....... In CRF mode, prevents VBV from lowering quality beyond this point. (from -1 to FLT_MAX) (default -1) -qp \u003cint\u003e E..V....... Constant quantization parameter rate control method (from -1 to INT_MAX) (default -1) -aq-mode \u003cint\u003e E..V....... AQ method (from -1 to INT_MAX) (default -1) ... ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:19:1","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"什么是 CRF？ CRF = Constant Rate Factor 在整个编码过程中，保持恒定的质量 在不太注重文件大小的情况下，适合以固定的质量存储视频 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:19:2","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"示例：转码为 H.264，Part 1 以固定质量（CRF）编码： ffmpeg -i \u003cinput\u003e -c:v libx264 -crf 23 -c:a aac -b:a 128k output.mkv 针对 H.264 编码，CRF 在 18 到 28 之间看起来“比较好”，CRF 值越低视频质量越好。（ HEVC 和 VP9 编码的 CRF 值不同） ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:19:3","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"示例：转码为 H.264，Part 2 双重编码（Two-pass）模式： ffmpeg -y -i \u003cinput\u003e -c:v libx264 -b:v 8M -pass 1 -c:a aac -b:a 128k -f mp4 /dev/null ffmpeg -i \u003cinput\u003e -c:v libx264 -b:v 8M -pass 2 -c:a aac -b:a 128k output.mp4 Windows 用户：用 nul 代替 /dev/null 更多细节，参考：https://trac.ffmpeg.org/wiki/Encode/H.264 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:19:4","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"码率控制 不同的控制方式： 固定码率（Constant Bitrate：CBR） 动态码率（Variable Bitrate：VBR） 平均码率（Average Bitrate：ABR） 恒定质量参数（Constant quantization parameter：CQP） 基于心里视觉特征的恒定质量，比如 x264/x265/libvpx-vp9 编码器中使用 CRF（Constant quality, based on psychovisual properties） 限制型码率（Constrained Bitrate：VBV） 具体在哪种情况下使用哪种码率模式，参考：https://slhck.info/video/2017/03/01/rate-control.html 重要说明：合适的码率取决于内容特征！ 译者按：对视频来说，码率并非越高越好，不同内容特征的视频对码率的要求并不一样，具体可以参考这篇文章：什么决定了你看到的画质 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:20:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"码率控制示例 不同码率控制模式下，帧大小随时间变化图（已平滑化处理） 注意 ABR 模式下，在一开始对码率的错误估计 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:20:1","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"速度 VS 质量 VS 文件大小（SPEED VS. QUALITY VS. FILE SIZE） 有损编码模式总是在这三者之间选择 例如： 你可以让视频质量又好，编码又快，但是输出文件会很大 你可以让视频质量又好，文件又小，但是编码速度会很慢 你可以让文件又小，编码又快，但是视频质量就会比较差 编者按：凡事需要做抉择的地方，不可能三角总在那儿等着你。视频编码时如何平衡速度、质量和文件大小，是最重要也是最复杂的课题之一，后面会学习/撰写/翻译更多的文章来介绍。 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:20:2","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"X264 编码器中的速度/质量预设 使用预设来选择 libx264 的编码速度 ffmpeg -i \u003cinput\u003e -c:v libx264 -crf 23 -preset ultrafast -an output.mkv ffmpeg -i \u003cinput\u003e -c:v libx264 -crf 23 -preset medium -an output.mkv ffmpeg -i \u003cinput\u003e -c:v libx264 -crf 23 -preset veryslow -an output.mkv 所有预设：ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow 同等质量下，一些编码速度于文件体积的对比样例： Preset Encoding Time File Size ultrafast 4.85s 15M medium 24.13s 5.2M veryslow 112.23s 4.9M ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:20:3","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"改变帧率 简单的通过丢弃帧和复制帧的方式来改变帧率： ffmpeg -i \u003cinput\u003e -r 24 \u003coutput\u003e 更复杂的方法涉及到过滤器（filter）参考 fps, mpdecimate, minterpolate filters: ffmpeg -i \u003cinput\u003e -filter:v fps=24 \u003coutput\u003e ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:21:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"轨道映射（STREAM MAPPING） 每一个文件都有一个唯一的 ID，从 0 开始 例如： 0:0 是第一个输入文件的第一条轨道（Steam） 0:1 是第一个输入文件的第二条轨道 2:a:0 是第三个输入文件的第一条音轨 … 你可以将输入文件的轨道映射到输出文件中（map input streams to output）例如：将音频添加到视频中： ffmpeg -i input.mp4 -i input.m4a -c copy -map 0:v:0 -map 1:a:0 output.mp4 更多细节请参考：http://trac.ffmpeg.org/wiki/Map ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:22:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"简单过滤器（SIMPLE FILTERING） FFmpeg 有很多视频、音频、字幕的过滤器（filter）： ffmpeg -i \u003cinput\u003e -filter:v \"\u003cfilter1\u003e,\u003cfilter2\u003e,\u003cfilter3\u003e\" \u003coutput\u003e 一个过滤器（滤镜）有一个名称，一些可选项，以及一些预设参数： -filter:v \u003cname\u003e=\u003coption1\u003e=\u003cvalue1\u003e:\u003coption2\u003e=\u003cvalue2\u003e ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:23:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"缩放（SCALING） 缩放到 320 x 240 ffmpeg -i \u003cinput\u003e -vf \"scale=w=320:h=240\" \u003coutput\u003e 缩放到高度 240 且保持纵横比可被 2 整除 ffmpeg -i \u003cinput\u003e -vf scale=w=-2:h=240 \u003coutput\u003e 缩放到 1280 x 720，如果需要的话缩放到更小 ffmpeg -i \u003cinput\u003e -vf \"scale=1280:720:force_original_aspect_ratio=decrease\" \u003coutput\u003e 更多关于缩放的技巧： http://trac.ffmpeg.org/wiki/Scaling%20(resizing)%20with%20ffmpeg https://superuser.com/questions/547296/ ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:23:1","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"填充（PADDING） 填充黑边到文件中，例如将 1920 x 800 的视频填充到 1920 x 1080： ffmpeg -i \u003cinput\u003e -vf \"pad=1920:1080:(ow-iw)/2:(oh-ih)/2\" \u003coutput\u003e 使用备注： 可以使用数学表达式 ow 和 oh 是输出文件的宽和高 iw 和 ih 是输入文件的宽和高 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:23:2","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"淡入淡出（FADING） 在特定的时间点使用指定持续时间的简单淡入淡出效果： ffmpeg -i \u003cinput\u003e -filter:v \\ \"fade=t=in:st=0:d=5,fade=t=out:st=30:d=5\" \\ \u003coutput\u003e 译者按：使用 FFmpeg 命令行时，有时一行代码太长不方便编写和查看，就可以使用换行符分行编写，不同的 Shell 有不同的换行符，例如 “\\” 是 Bash 的换行符，Windows 下 CMD 的换行符是 “^”，请注意替换。 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:23:3","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"文本绘图 / 文字水印（DRAWING TEXT） 用于在视频上加上文本的复杂系统： ffmpeg -i \u003cinput\u003e -vf \\ drawtext=\"text='Test Text':x=100:y=50:\\ fontsize=24:fontcolor=yellow:box=1:boxcolor=red\" \\ \u003coutput\u003e 字体、大小、位置、颜色…都有都有各种相关的参数 文字烧录功能可以拓展，例如按帧数或者时间码烧录 更多详细信息，参考：http://ffmpeg.org/ffmpeg-all.html#drawtext-1 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:23:4","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"复杂过滤器（COMPLEX FILTERING） 复杂过滤器不止一个输入或者输出： ffmpeg -i \u003cinput1\u003e -i \u003cinput2\u003e -filter_complex \\ \"[0:v:0][1:v:0]overlay[outv]\" \\ -map \"[outv]\" \u003coutput\u003e 过程拆解： 指定输入到过滤链（filterchain）（例如 [0:v:0][1:v:0]） 指定过滤链中的滤镜（例如：overlay） 指定过滤链的输出标签（例如：[outv]） 映射输出标签到输出文件 使用 ; 一次可以同时使用多个过滤链 更多详细内容，参考：http://ffmpeg.org/ffmpeg-all.html#Filtergraph-syntax-1 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:24:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"拼接轨道（CONCATENATING STREAMS） 解码三个视频/音频轨道并拼接到另一个中： ffmpeg -i \u003cinput1\u003e -i \u003cinput2\u003e -i \u003cinput3\u003e -filter_complex \\ \"[0:0][0:1][1:0][1:1][2:0][2:1]concat=n=3:v=1:a=1[outv][outa]\" \\ -map \"[outv]\" -map \"[outa]\" \u003coutput\u003e 更多信息，参考：http://trac.ffmpeg.org/wiki/Concatenate（也适用于其他方法） ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:24:1","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"时间线编辑（TIMELINE EDITING） 仅在特定时间点启用过滤器 例如： 在左上角显示水印 仅在第 1 和第 2 秒显示 ffmpeg -i \u003cvideo\u003e -i \u003cwatermark\u003e -filter_complex \\ \"[0:v][1:v]overlay=10:10:enable='between(t,1,2)'[outv]\" \\ -map \"[outv]\" \u003coutput\u003e 更多用法，请参考：http://ffmpeg.org/ffmpeg-all.html#Timeline-editing ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:24:2","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"其他过滤器 除了前面介绍的那些过滤器，ffmpeg 生态中还有很多很多其他的过滤器。 例如： 使用选择过滤器检测场景变化 去除水印（delogo） 模糊、边缘检测和卷积滤波器 视频稳定 矢量示波器、直方图和其他信息 色度和 alpha 控制 字幕编辑 … ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:25:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"计算简单的质量指标 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:26:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"PSNR (Peak Signal To Noise Ratio) $ ffmpeg -i \u003cdegraded\u003e -i \u003creference\u003e -filter_complex psnr -f null /dev/null [Parsed_psnr_0 @ 0x7fdb187045c0] PSNR y:33.437789 u:39.814416 v:39.319141 average:34.698320 min:29.305186 max:inf 译者按：PSNR 中文标准译名为“峰值信噪比”，是一个比较成熟的衡量压缩影像信噪比的指标，详情可以参考 中文维基百科：峰值信噪比 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:26:1","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"SSIM (Structural Similarity) $ ffmpeg -i \u003cdegraded\u003e -i \u003creference\u003e -filter_complex ssim -f null /dev/null [Parsed_ssim_0 @ 0x7fbf0500b660] SSIM Y:0.925477 (11.277116) U:0.948906 (12.916325) V:0.946795 (12.740513) All:0.932935 (11.735054) 译者按：SSIM 中文一般译为“结构相似性”，是衡量两幅图像相似度的指标，其详细的科普和使用可以参考这篇文章：使用SSIM量化评价视频编码质量 译者按：除了原作者列出的 PSNR 和 SSIM 两个指标外，还有一个 Netflix 和 南加州大学联合开发的 VMAF （Video Multimethod Assessment Fusion）是目前 Netflix 主要使用的指标，详情还是可以参考前文提到的这篇文章：什么决定了你看到的画质 使用 ffprobe 获取媒体信息 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:26:2","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"基本概念 基础语法： ffprobe \u003cinput\u003e [-select_streams \u003cselection\u003e] [-show_streams|-show_format|-show_frames|-show_packets] [-show_entries \u003centries\u003e] [-of \u003coutput-format\u003e] 解释： select_streams 用于指定选择视频或者音频 show_ 用于选择显示哪种类型的信息 show_entries 用于选择显示更少的信息 of 用于设置输出格式 更多信息，参考： https://ffmpeg.org/ffprobe.html http://trac.ffmpeg.org/wiki/FFprobeTips ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:27:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"实际 ffpobe 案例 Part 1 显示所有的流： ffprobe \u003cinput\u003e -show_streams 显示视频流信息： ffprobe \u003cinput\u003e -select_streams v -show_format Show presentation timestamp and frame type of every frame, in CSV format (p=0 disables CSV section headers): ffprobe \u003cinput\u003e -select_streams v -show_frames \\ -show_entries frame=pkt_pts_time,pict_type -of csv=p=0 译者按：第三个案例不太懂，我自己试了试，效果也有点奇怪，先放着吧 😂 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:28:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"实际 ffpobe 案例 Part 2 更改输出为 JSON 格式： ffprobe \u003cinput\u003e -select_streams v -show_packets -of json 获取文件中流的编号（nk = 1 禁用 keys） ffprobe \u003cinput\u003e -show_format -show_entries format=nb_streams -of compact=nk=1:p=0 以秒或 HH:MM:SS.ms 为单位获取视频长度 ffprobe \u003cinput\u003e -show_format -show_entries format=duration -of compact=nk=1:p=0 ffprobe -sexagesimal \u003cinput\u003e -show_format -show_entries format=duration -of compact=nk=1:p=0 以 Bit/s 为单位获取音频流 ffprobe \u003cinput\u003e -select_streams a -show_entries stream=bit_rate -of compact=nk=1:p=0 译者按：其实这一章节的案例我个人觉得没有很大作用，平时查看媒体文件的信息基本上使用 ffprobe \u003cinput\u003e -hide_banner 就够了。 检查视频编解码器 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:29:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"调试运动向量 在 FFmpeg 中使用 H.264 编解码器可视化运动向量的简单方法（不适用于其他编解码器） ffplay -flags2 +export_mvs input.mp4 -vf codecview=mv=pf+bf+bb ffmpeg -flags2 +export_mvs -i input.mp4 -vf codecview=mv=pf+bf+bb \u003coutput\u003e pf – P 帧的向前预测运动向量 bf – B 帧的向前预测运动向量 bb – B 帧的向后预测运动向量 跟多参考：http://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors 译者按：运动向量这方面的知识点我也不懂，来日再补充吧 😂 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:30:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Media"],"content":"视频流分析 图形化方式分析视频流的其他软件： Elecard Stream Analyzer (商业) CodecVisa (商业) Intel Video Pro Analyzer (商业) AOMAnalyzer (免费，针对 AV1/VP9 编码的视频) 总结 通过这节课程，你应该学到了以下知识点： 理解 FFmpeg 库，编解码器，容器，编码器… 编码视频和音频 使用基础的过滤器 读取流信息和元数据 如果你遇到了困难，会寻找帮助 ","date":"2021-06-12","objectID":"/ffmpeg-encoding-and-editing-course/:31:0","tags":["FFmpeg"],"title":"FFmpeg 编码和编辑入门","uri":"/ffmpeg-encoding-and-editing-course/"},{"categories":["Program Language"],"content":"Overview 容器，就是可以容纳其它对象的对象，从 JDK 1.2 开始，Java 提供了 Java Collection Framework（JCF） 给开发者提供了一个通用的容器框架。 容器中只能放对象，对于基本类型（int、long、float 等）需要包装成对应的对象（Integer、Long、Float）才能放入容器。大多数时候装箱和拆箱都是自动完成的，这会造成一定程度上的性能和空间开销，但是简化了设计和编码，提高了开发效率。使用容器大概有如下优点： 降低学习难度 降低编程难度 降低设计和实现相关 API 的难度 提高程序可复用性 提高 API 之间的互操作性 提高程序性能（容器的底层数据结构和算法通常过了大规模的功能和性能验证，大部分场景下性能会比我们自己写的好） Java 集合框架分为 Collection 和 Map 两大类，Collection 是存储单元素的容器，Map 是存储键值对（两个对象）的容器。 详情的 JCF 类图如下，包含并发和非并发实现，以及底层数据结构： 在 J.U.C(java.util.concurrent) 提供并发容器之前，所有容器都在 java.util 包下，其中大部分都是非线程安全的，这意味着多个线程并发读写时可能会出现数据不一致的情况。 即使提供了线程安全的实现，也是比较简单粗暴的使用 Synchronized 关键字对操作加锁，比如线程安全的 Map 容器：HashTable，和线程安全的 Collection 容器：Vector，实际上这些容器在并发读写的需求下，只能保证安全，不能保证高效，因此基本都已经不推荐使用了。 ","date":"2021-05-02","objectID":"/the-java-collections-framework/:1:0","tags":["Collections","Java"],"title":"Java 集合框架一文通","uri":"/the-java-collections-framework/"},{"categories":["Program Language"],"content":"Non Concurrent Collections 我们先来看一下实际开发中使用得最多的 “单线程容器”，其类图结构大概如下图： ","date":"2021-05-02","objectID":"/the-java-collections-framework/:2:0","tags":["Collections","Java"],"title":"Java 集合框架一文通","uri":"/the-java-collections-framework/"},{"categories":["Program Language"],"content":"Collection Collection 接口是 JDK 中所有单元素容器的祖先接口，其下主要有三个接口，分别是可重复集合容器接口 List，不可重复集合容器接口 Set，和队列容器接口 Queue，分别适用于不同场景。 List List 是一个元素有序、可重复、可为空的集合接口，集合中每个元素都有对应的顺序索引，默认按照元素的添加顺序设置下标，可以通过下标访问指定位置的元素，但是由于不同实现类中底层使用的数据结构不同，添加、删除和随机访问的时间复杂度各不相同。 ArrayList ArrayList 是基于数组实现的容器类，每个 ArrayList 都有一个容量（capacity）表示底层数组的大小，当容量不足时，ArrayList 会自动扩容，默认扩容步长是当前容量的 1.5 倍。 由于 ArrayList 的底层数据结构是数组，需要连续的内存空间，所以自动扩容实际上是申请一个原来大小 1.5 倍的新数组，然后将原数组中的数据拷贝到新数组中，这个操作是比较耗时的。 因此在使用 ArrayList 时，可以使用如下两个技巧，尽量避免或减少扩容操作，提高效率： 在初始化时，尽量设置一个合理的初始大小，减少扩容频率。 在添加大量元素之前，可以手动调用 ensureCapacity(int minCapacity) 增大容量，减少递增式扩容的次数。 基于数组的数据结构，ArrayList 随机访问的的效率非常高，时间复杂度是 $O(1)$，而插入和删除的效率稍低，时间复杂度是 $O(n)$ ArrayList 详细的源码解析参考这里 LinkedList LinkedList 是基于链表实现的容器类，同时实现了 List 接口和 Queue 接口的集合，这意味它既可以当做顺序容器（List）使用，又可以当做队列（Queue）使用，还可以当做栈（Stack）来使用。 实际上，当我们需要栈或者队列时，应该首选 ArrayDeque，在栈和队列的使用场景中，ArrayDeque 比 LinkedList 性能更好。所以 LinkedList 更多的时候还是用来当做顺序集合（List）来使用。 基于链表的数据结构，LinkedList 插入和删除的效率非常高，时间复杂度是 $O(1)$，而随机访问的效率稍低，时间复杂度是 $O(n)$，而且不存在扩容时的数据拷贝导致的效率问题。 LinkedList 详细的源码解析，参考这里 Vector Vector 基本和 ArrayList 基本一样，但是内部使用 synchronized 对所有读写操作都做了同步，因此是线程安全的，但是应为 synchronized 本身实现线程安全的方式效率并不高，所以在多线程高并发场景下，使用 Vector 可能会造成性能问题，应该使用 JUC 中提供的 CopyOnWriteArrayList。 Stack Stack 继承自 Vector，是一个使用数组实现的栈结构，由于功能和性能限制，现在已经不推荐使用了，如果需要栈结构，应该使用 Queue 接口下的 ArrayDeque 实现类，参考下文 ArrayDeque 的介绍。 Set Set 接口中的方法基本和 List 一样，区别在于 Set 集合中不允许重复元素，如果添加相同的元素到 Set 集合中，第二次添加时 add() 方法会返回 false，数据会添加失败，Set 接口也不强制保证集合中元素的顺序，不同的实现类根据自身的实现方式决定是否保持顺序，并且也不限制元素是否可为空，不同的实现类可否为空的特点不同。 Set 接口的实现其实就是把对应的 Map 接口的实现进行一层包装，比如 HashSet 是对 HashMap 的包装，TreeSet 是对 TreeMap 的包装，LinkedHashSet 是对 LinkedHashMap 的包装。 HashSet HashSet 实际上是对 HashMap 的一个包装，如下代码所示： public class HashSet\u003cE\u003e { ...... //HashSet里面有一个HashMap private transient HashMap\u003cE,Object\u003e map; // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object(); public HashSet() { map = new HashMap\u003c\u003e(); } ...... public boolean add(E e) { //简单的方法转换 return map.put(e, PRESENT)==null; } ...... } 所以其实现原理和 HashMap 一致，参考下文 HashMap 的实现与特点。 TreeSet TreeSet 实际上也是对 TreeMap 的一个包装，如下代码： public class TreeSet\u003cE\u003e extends AbstractSet\u003cE\u003e implements NavigableSet\u003cE\u003e, Cloneable, java.io.Serializable { ...... private transient NavigableMap\u003cE,Object\u003e m; // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object(); public TreeSet() { // TreeSet里面有一个TreeMap this.m = new TreeMap\u003cE,Object\u003e(); } ...... public boolean add(E e) { //简单的方法转换 return m.put(e, PRESENT)==null; } ...... } 所以其实现原理和 TreeMap 一致，参考下文 TreeMap 的实现与特点。 LinkedHashSet LinkedHashSet 实际上也是对 LinkedHashMap 的包装，基本就是这样： public class LinkedHashSet\u003cE\u003e extends HashSet\u003cE\u003e implements Set\u003cE\u003e, Cloneable, java.io.Serializable { ...... // LinkedHashSet里面有一个LinkedHashMap public LinkedHashSet(int initialCapacity, float loadFactor) { map = new LinkedHashMap\u003c\u003e(initialCapacity, loadFactor); } ...... //简单的方法转换 public boolean add(E e) { return map.put(e, PRESENT)==null; } ...... } 所以其实现原理和 LinkedHashMap 一致，参考下文 LinkedHashMap 的实现与特点。 Queue Queue 是 JDK 中队列也继承自 Collection 接口，除了 Collection 接口中的方法，还额外提供了两组共 6 个方法，规范队列特征的能力，一组是抛出异常的实现，一组是返回值的实现（没有则返回 null） hrows exception Returns special value Insert add(e) offer(e) Remove remove(e) poll(e) Examine element(e) peek(e) Deque \u0026 ArrayDeque Deque 是（double ended queue）的简称，顾名思义就是双端队列，是继承自 Queue 接口的子接口。 我们知道，在数据结构中，队列（Queue）和栈（Stack）的是很相似的，队列的出入口分别在两端，所以是先进先出（FIFO），而栈的出入口在同一端，所以是后进先出（LIFO）。而双端队列的两端分别都可以做出入口，如果封住其中的一端，那就变成了栈，所以双端队列也可以直接当做栈来用。 当做队列使用时，下表列出了 Deque 与 Queue 相对应的接口： Queue Method Equivalent Deque Method 说明 add(e) addLast(e) 向队尾插入元素，失败则抛出异常 offer(e) offerLast(e) 向队尾插入元素，失败则返回false remove() removeFirst() 获取并删除队首元素，失败则抛出异常 poll() pollFirst() 获取并删除队首元素，失败则返回null element() getFirst() 获取但不删除队首元素，失败则抛出异常 peek() peekFirst() 获取但不删除队首元素，失败则返回null 当做栈使用时，下表列出了 Deque 和 与 Stack（已退休的栈结构实现）对应的接口： Stack Method Equivalent Deque Method 说明 push(e) addFirst(e) 向栈顶插入元素，失败则抛出异常 无 offerFirst(e) 向栈顶插入元素，失败则返回false pop() removeFirst() 获取并删除栈顶元素，失败则抛出异常 无 pollFi","date":"2021-05-02","objectID":"/the-java-collections-framework/:2:1","tags":["Collections","Java"],"title":"Java 集合框架一文通","uri":"/the-java-collections-framework/"},{"categories":["Program Language"],"content":"Map Map 接口可以看作是和 Collection 平级的借口，是 JDK 中所有键值对容器的祖先接口，按照是否保持容器中元素的顺序，大概可以分为两类： 实现了 SortedMap 接口的“有序 Map”，以及一个特殊的 EnumMap 没有实现 SortedMap 接口的 “无序 Map” HashMap HashMap 是 Map 接口最重要的实现类之一，也是日常 Java 开发中最常用的键值对数据结构之一。可以放入 key 为 null 的元素，也可以放入 value 为 null 的元素。 HashMap 是基于 散列表（Hash Table） 数据结构的容器，但是散列表必须要解决散列冲突的问题，散列冲突目前主要有两种解决方案：开放寻址法（open addressing）和链表法（chaining），HashMap 使用链表法解决散列冲突问题。 理论上 HashMap 的读、写、删除的效率都是 $O(1)$，非常高效，但是由于散列冲突问题的存在，HashMap 实际性能表现可能不太稳定，而且不一定能达到理论效率。 影响 HashMap 性能的两个最重要的参数是：初始容量（initial capacity）和负载系数（load factory） 初始容量指定了初始 table 的大小，复杂系数指定了 bucket 自动扩容的临界值。当 entry 的个数超过 capacity * load_factory 时，容器将自动扩容并重新散列。对于插入元素较多的 HashMap，将初始容量设置得大一些可以减少自动扩容（同 ArrayList 的原理）和重新散列的次数。 为了优化 HashMap 的实际性能，其具体实现在 JDK 8 前后有一些差别。 HashMap 详细的源码解析，参考这里 Before JDK 8 在 JDK 8 之前，HashMap 底层用数据结构是数组+链表，用数组做 buckets，实际数据存储在每个 bucket 后的链表中，其大致结构如下： 这是非常经典的散列冲突解决方案，但是在数据量很大或者散列不够均匀的时候，容易导致数据节点链表过长，会降低散列表的读取性能，达不到理想状态下的 $O(1)$ 时间复杂度，甚至是达到 $O(n)$ 级别。 After JDK 8 在 JDK 8（含）之后，HashMap 最大的变化就在底层数据结构上引入了红黑树，为了降低数据节点的查询开销，在节点数据达到 8 个的时候，会将链表转为红黑树，这样在节点数据比较多的时候，时间复杂度可以降到 $O(log_n)$ 其结构示意图如下： 红黑树是一个性能非常稳定的近似平衡二叉查找树（binary seach tree）结构，当一个节点的数据确实比较多（可能是因为散列函数设计得不好，也可能是数据量实在太大）的时候，转成红黑树结构是一个非常靠谱的提升 HashMap 整体性能的方法。 虽然红黑树实现起来比链表复杂的多，但是好在 JDK 的工程师们已经帮我们最好了这部分的工作，我们只需要升级到 JDK 8，就可以不改一处代码，享受 HashMap 的性能提升。 HashTable HashTable 基本和 HashMap 差不多，只是内部各个方法用 Synchronized 实现了同步，在多线程读写的情况下，不会出现数据不一致。 但是 HashTable 多线程读写的性能并不好，所以，如果确实需要多线程读写安全的 Map 容器，应该使用 J.U.C 提供的 ConcurrentMap 接口的实现类，比如 ConcurrentHashMap，而不应该使用 HashTable，HashTable 该退休了。 LinkedHashMap LinkedHashMap 是 HashMap 的直接子类，所以 key 和 value 同样可以为空。看这个名字大概就能猜到，这是 LinkedList 和 HashMap 的结合，可以将 LinkedHashMap 看作是用 LinkedList 强化过的 HashMap。如下是 LinkedHashMap 的结构图： 从图上能看出来，LinkedHashMap 和 HashMap 主体结构上完全一致，区别在于 LinkedHashMap 使用双向链表实现了冲突链表，并且这个双向链表将所有的 entry 都连了起来，这样做相比 HashMap 有两个额外的好处： 可以保证元素的迭代顺序和插入顺序相同。 迭代整个 Map 的时候，不需要像 HashMap 那样迭代整个 table，而是只需要遍历 header 指向的双向链表即可，也就是 LinkedHashMap 的迭代时间和 Table 的大小无关，而是只与实际上 entry 的数量有关。 由于主体结构和 HashMap 一样，所以影响 LinkedHashMap 性能的也是初始容量（initial capacity）和负载系数，原因也和 HashMap 一样。 LinkedHashMap 除了保证迭代顺序之外，还有一个非常有用的用法：轻松实现一个先进先出（FIFO）策略的缓存结构，注意，是缓存，不是队列。 LinkedHashMap 有一个 HashMap 中没有的子类方法：boolean removeEldestEntry(Map.Entry\u003cK,V\u003e eldest)，该方法的作用是告诉 Map 是否要删除“最老”的 Entry，所谓最老就是当前 Map 中最早插入的 Entry，如果该方法返回 true，最老的那个元素就会被删除。而且在每次插入新元素的之后LinkedHashMap 会自动询问 removeEldestEntry() 是否要删除最老的元素。这样只需要在子类中重载该方法，当元素个数超过一定数量时让removeEldestEntry() 返回 true，就能够实现一个固定大小的FIFO策略的缓存。示例代码如下： /** 一个固定大小的 FIFO 替换策略的缓存 */ class FIFOCache\u003cK, V\u003e extends LinkedHashMap\u003cK, V\u003e{ private final int cacheSize; public FIFOCache(int cacheSize){ this.cacheSize = cacheSize; } // 当 Entry 个数超过 cacheSize 时，删除最老的 Entry @Override protected boolean removeEldestEntry(Map.Entry\u003cK,V\u003e eldest) { return size() \u003e cacheSize; } } LinkedHashMap 更加详细的源码解析，参考 这里 TreeMap TreeMap 实现了 SortedMap 接口，意味着容器内部会按照 key 的大小对 Map 中的元素进行排序，而 key 大小的判断，既可以通过其自身的自然顺序（natural ordering），也可以通过构造时传入的比较器 Comparator。 TreeMap 底层使用红黑树（Red-Black Tree）实现，这意味着其读取、插入和删除的时间复杂度都是 $O(log_n)$，而且得益于红黑树独特的结构，TreeMap 的性能稳定性也很好。其结构大致如下图： 出于性能原因，TreeMap 是非同步的，这意味着它不能在多线程读写情况下使用，如果需要“多线程安全的 TreeMap”，可以用 Collections.synchronizedSortedMap 做如下包装： SortedMap m = Collections.synchronizedSortedMap(new TreeMap(...)); 同样还是由于 Synchronized 本身性能开销较大，不建议使用这种方式。所以，如果有这样的需求，可以直接使用 J.U.C 包中提供的 ConcurrentSkipListMap 实现。 如上 HashMap 中所述，红黑树本身的实现和操作比较复杂，涉及到各种左旋右旋和颜色调整，详细源码分析参考这里 EnumMap EnumMap 是一个有点特别的 Map 接口实现类，他是一个针对枚举（enum）类型 key 优化过的 Map，在要存储的键值对所有的 key 都是 enum 类型的时候，使用 EnumMap 会比 HashMap 更高效。 我们知道 HashMap 是通过散列函数计算 key 的值，然后储存到数组中，这个过程中就会产生两个可能会影响性能的点：散列函数性能和散列冲突的解决。 如果我们要存储的数据的 key 都是 enum 类型的话，编译器会为每个枚举类型生成的常量序列号，也就是 ordinal，这个值是不会冲突的，那么就只需要将这个值设置为数组的下标就可以了，这样一来就可以直接避免使用散列函数，就可以大幅提高读写性能，读、写、删除的时间复杂度都是 $O(1)$ 所以，如果要存储到 Map 中的的数据 key 都是 enum 类型，建议使用 EnumMap 代替 HashMap，可以有效提高综合性能。 WeakHashMap 如果说 EnumMap 是个有点特别的 Map，那么 WeekHashMap 就是十分特别的 Map。它的特别之处在于 WeekHashMap 中的元素（entry）可能随时被 GC 自动删除，即便我们没有手动的调用 remove() 或者 clear() 函数。 更直观的来说，即便我们没有显示的删除其中的元素，也有可能随时发生如下的情况： 调用两次size()方法返回不同的值； 两次调用isEmpty()方法","date":"2021-05-02","objectID":"/the-java-collections-framework/:2:2","tags":["Collections","Java"],"title":"Java 集合框架一文通","uri":"/the-java-collections-framework/"},{"categories":["Program Language"],"content":"Concurrent Collections J.U.C 包的出现给 Java 并发编程效率带来了巨大的提升，其中就提供了不少在并发情况下既保证数据安全，又保证效率的集合，让古老的 Vector 和 HashTable 直接退休！ 容我先学学再来更新 :) 本文主要参考 Java 全栈知识体系 - Java集合框架 和 Oracle Java Docs - Collections Framework Overview ","date":"2021-05-02","objectID":"/the-java-collections-framework/:3:0","tags":["Collections","Java"],"title":"Java 集合框架一文通","uri":"/the-java-collections-framework/"},{"categories":["Database"],"content":"Clickhouse 在离线数据分析技术领域早已声名远扬，如果还不知道可以 点这里 了解下。 最近由于项目需求使用到了 clickhouse 做分析数据库，于是用测试环境做了一个单表 6 亿数据量的性能测试，记录一下测试结果，有做超大数据量分析技术选型需求的朋友可以参考下。 ","date":"2021-03-16","objectID":"/clickhouse-billion-level-data-performance-test/:0:0","tags":["BigData","Clickhouse","OLAP"],"title":"Clickhouse 亿级数据性能测试","uri":"/clickhouse-billion-level-data-performance-test/"},{"categories":["Database"],"content":"服务器信息 CPU：Intel Xeon Gold 6240 @ 8x 2.594GHz 内存：32G 系统：CentOS 7.6 Linux内核版本：3.10.0 磁盘类型：机械硬盘 文件系统：ext4 ","date":"2021-03-16","objectID":"/clickhouse-billion-level-data-performance-test/:1:0","tags":["BigData","Clickhouse","OLAP"],"title":"Clickhouse 亿级数据性能测试","uri":"/clickhouse-billion-level-data-performance-test/"},{"categories":["Database"],"content":"Clickhouse 信息 部署方式：单机部署 版本：20.8.11.17 ","date":"2021-03-16","objectID":"/clickhouse-billion-level-data-performance-test/:2:0","tags":["BigData","Clickhouse","OLAP"],"title":"Clickhouse 亿级数据性能测试","uri":"/clickhouse-billion-level-data-performance-test/"},{"categories":["Database"],"content":"测试情况 测试数据和测试方法来自 clickshouse 官方的 Star Schema Benchmark 按照官方指导造出了测试数据之后，先看一下数据量和空间占用情况。 ","date":"2021-03-16","objectID":"/clickhouse-billion-level-data-performance-test/:3:0","tags":["BigData","Clickhouse","OLAP"],"title":"Clickhouse 亿级数据性能测试","uri":"/clickhouse-billion-level-data-performance-test/"},{"categories":["Database"],"content":"数据量和空间占用 表名 列数 数据行数 原始大小 压缩大小 压缩率 supplier 6 200,000 11.07 MiB 7.53 MiB 68 customer 7 3,000,000 168.83 MiB 114.72 MiB 68 part 8 1,400,000 34.29 MiB 24.08 MiB 70 lineorder 16 600,037,902 24.03 GiB 16.67 GiB 69 lineorder_flat 37 688,552,212 111.38 GiB 61.05 GiB 55 可以看到 clickhouse 的压缩率很高，压缩率都在 50 以上，基本可以达到 70 左右。数据体积的减小可以非常有效的减少磁盘空间占用、提高 I/O 性能，这对整体查询性能的提升非常有效。 supplier、customer、part、lineorder 为一个简单的「供应商-客户-订单-地区」的星型模型，lineorder_flat 为根据这个星型模型数据关系合并的大宽表，所有分析都直接在这张大宽表中执行，减少不必要的表关联，符合我们实际工作中的分析建表逻辑。 以下性能测试的所有分析 SQL 都在这张大宽表中运行，未进行表关联查询。 ","date":"2021-03-16","objectID":"/clickhouse-billion-level-data-performance-test/:3:1","tags":["BigData","Clickhouse","OLAP"],"title":"Clickhouse 亿级数据性能测试","uri":"/clickhouse-billion-level-data-performance-test/"},{"categories":["Database"],"content":"查询性能测试详情 Query 1.1 SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue FROM lineorder_flat WHERE (toYear(LO_ORDERDATE) = 1993) AND ((LO_DISCOUNT \u003e= 1) AND (LO_DISCOUNT \u003c= 3)) AND (LO_QUANTITY \u003c 25) ┌────────revenue─┐ │ 44652567249651 │ └────────────────┘ 1 rows in set. Elapsed: 0.242 sec. Processed 91.01 million rows, 728.06 MB (375.91 million rows/s., 3.01 GB/s.) 扫描行数：91,010,000 大约9100万 耗时(秒)：0.242 查询列数：2 结果行数：1 Query 1.2 SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue FROM lineorder_flat WHERE (toYYYYMM(LO_ORDERDATE) = 199401) AND ((LO_DISCOUNT \u003e= 4) AND (LO_DISCOUNT \u003c= 6)) AND ((LO_QUANTITY \u003e= 26) AND (LO_QUANTITY \u003c= 35)) ┌───────revenue─┐ │ 9624332170119 │ └───────────────┘ 1 rows in set. Elapsed: 0.040 sec. Processed 7.75 million rows, 61.96 MB (191.44 million rows/s., 1.53 GB/s.) 扫描行数：7,750,000 775万 耗时(秒)：0.040 查询列数：2 返回行数：1 Query 2.1 SELECT sum(LO_REVENUE), toYear(LO_ORDERDATE) AS year, P_BRAND FROM lineorder_flat WHERE (P_CATEGORY = 'MFGR#12') AND (S_REGION = 'AMERICA') GROUP BY year, P_BRAND ORDER BY year ASC, P_BRAND ASC ┌─sum(LO_REVENUE)─┬─year─┬─P_BRAND───┐ │ 64420005618 │ 1992 │ MFGR#121 │ │ 63389346096 │ 1992 │ MFGR#1210 │ │ ........... │ .... │ ..........│ │ 39679892915 │ 1998 │ MFGR#128 │ │ 35300513083 │ 1998 │ MFGR#129 │ └─────────────────┴──────┴───────────┘ 280 rows in set. Elapsed: 8.558 sec. Processed 600.04 million rows, 6.20 GB (70.11 million rows/s., 725.04 MB/s.) 扫描行数：600,040,000 大约6亿 耗时(秒)：8.558 查询列数：3 结果行数：280 Query 2.2 SELECT sum(LO_REVENUE), toYear(LO_ORDERDATE) AS year, P_BRAND FROM lineorder_flat WHERE ((P_BRAND \u003e= 'MFGR#2221') AND (P_BRAND \u003c= 'MFGR#2228')) AND (S_REGION = 'ASIA') GROUP BY year, P_BRAND ORDER BY year ASC, P_BRAND ASC ┌─sum(LO_REVENUE)─┬─year─┬─P_BRAND───┐ │ 66450349438 │ 1992 │ MFGR#2221 │ │ 65423264312 │ 1992 │ MFGR#2222 │ │ ........... │ .... │ ......... │ │ 39907545239 │ 1998 │ MFGR#2227 │ │ 40654201840 │ 1998 │ MFGR#2228 │ └─────────────────┴──────┴───────────┘ 56 rows in set. Elapsed: 1.242 sec. Processed 600.04 million rows, 5.60 GB (482.97 million rows/s., 4.51 GB/s.) 扫描行数：600,040,000 大约6亿 耗时(秒)：1.242 查询列数：3 结果行数：56 Query 3.1 SELECT C_NATION, S_NATION, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenue FROM lineorder_flat WHERE (C_REGION = 'ASIA') AND (S_REGION = 'ASIA') AND (year \u003e= 1992) AND (year \u003c= 1997) GROUP BY C_NATION, S_NATION, year ORDER BY year ASC, revenue DESC ┌─C_NATION──┬─S_NATION──┬─year─┬──────revenue─┐ │ INDIA │ INDIA │ 1992 │ 537778456208 │ │ INDONESIA │ INDIA │ 1992 │ 536684093041 │ │ ..... │ ....... │ .... │ ............ │ │ CHINA │ CHINA │ 1997 │ 525562838002 │ │ JAPAN │ VIETNAM │ 1997 │ 525495763677 │ └───────────┴───────────┴──────┴──────────────┘ 150 rows in set. Elapsed: 3.533 sec. Processed 546.67 million rows, 5.48 GB (154.72 million rows/s., 1.55 GB/s.) 扫描行数：546,670,000 大约5亿4千多万 耗时(秒)：3.533 查询列数：4 结果行数：150 Query 3.2 SELECT C_CITY, S_CITY, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenue FROM lineorder_flat WHERE (C_NATION = 'UNITED STATES') AND (S_NATION = 'UNITED STATES') AND (year \u003e= 1992) AND (year \u003c= 1997) GROUP BY C_CITY, S_CITY, year ORDER BY year ASC, revenue DESC ┌─C_CITY─────┬─S_CITY─────┬─year─┬────revenue─┐ │ UNITED ST6 │ UNITED ST6 │ 1992 │ 5694246807 │ │ UNITED ST0 │ UNITED ST0 │ 1992 │ 5676049026 │ │ .......... │ .......... │ .... │ .......... │ │ UNITED ST9 │ UNITED ST9 │ 1997 │ 4836163349 │ │ UNITED ST9 │ UNITED ST5 │ 1997 │ 4769919410 │ └────────────┴────────────┴──────┴────────────┘ 600 rows in set. Elapsed: 1.000 sec. Processed 546.67 million rows, 5.56 GB (546.59 million rows/s., 5.56 GB/s.) 扫描行数：546,670,000 大约5亿4千多万 耗时(秒)：1.00 查询列数：4 结果行数：600 Query 4.1 SELECT toYear(LO_ORDERDATE) AS year, C_NATION, sum(LO_REVENUE - LO_SUPPLYCOST) AS profit FROM lineorder_flat WHERE (C_REGION = 'AMERICA') AND (S_REGION = 'AMERICA') AND ((P_MFGR = 'MFGR#1') OR (P_MFGR = 'MFGR#2')) GROUP BY year, C_NATION ORDER BY year ASC, C_NATION ASC ┌─year─┬─C_NATION──────┬────────profit─┐ │ 1992 │ ARGENTINA │ 1041983042066 │ │ 1992 │","date":"2021-03-16","objectID":"/clickhouse-billion-level-data-performance-test/:3:2","tags":["BigData","Clickhouse","OLAP"],"title":"Clickhouse 亿级数据性能测试","uri":"/clickhouse-billion-level-data-performance-test/"},{"categories":["Database"],"content":"性能测试结果汇总 查询语句 SQL简要说明 扫描行数 返回行数 查询列数 耗时(秒) Q1.1 乘积、汇总、4个条件、首次运行 91,010,000 1 2 0.242 Q1.2 Q1.1增加1个条件运行 7,750,000 1 2 0.040 Q2.1 汇总、函数、2列分组、2列排序、首次运行 600,040,000 280 3 8.558 Q2.2 Q2.1增加1个条件运行 600,040,000 56 3 1.242 Q3.1 汇总、函数、3列分组、2列排序、首次运行 546,670,000 150 4 3.533 Q3.2 Q3.1更换条件运行 546,670,000 600 4 1 Q4.1 相减、汇总、函数、2列分组、2列排序、首次运行 600,040,000 35 4 5.006 Q4.2 Q4.1增加2个条件运行 144,420,000 100 4 0.826 在当前软硬件环境下，扫描 6 亿多行数据，常见的分析语句首次运行最慢在 8 秒左右能返回结果，相同的分析逻辑更换条件再次查询的时候效率有明显的提升，可以缩短到 1 秒左右，如果只是简单的列查询没有加减乘除、聚合等逻辑，扫描全表 6 亿多行数据首次查询基本可以在 2 秒内执行完成。 以上是单机部署 Clickhouse 的性能表现情况，在不提升单机硬件性能的条件下，如果使用合理的集群部署，性能还能进一步提高。 这个表现，毋庸置疑是目前离线数据分析数据库领域的性能王者。 ","date":"2021-03-16","objectID":"/clickhouse-billion-level-data-performance-test/:4:0","tags":["BigData","Clickhouse","OLAP"],"title":"Clickhouse 亿级数据性能测试","uri":"/clickhouse-billion-level-data-performance-test/"},{"categories":["Computer Science"],"content":"简介 数组是一个线性表类型的数据数据，它用一组连续的内存空间，来存储一组类型相同的数据。 数组中使用下标来表示某个元素在数组中的位置，下标从 0 开始计数。例如数组 array 中第 6 个元素，表示为：array[5] ","date":"2021-03-02","objectID":"/data-structures-array/:1:0","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"读取 ","date":"2021-03-02","objectID":"/data-structures-array/:2:0","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"随机访问 基于连续内存空间和类型相同两个特征，使得随机访问数组中的数据成为可能。 假设数组 array 的起始内存地址为 start，因为储存的数据类型相同，因此每块数据占用的内存大小也相同，假设为 size，那么读取下标为 i 的数据的内存地址的公式为：array[i] = start + i * size 计算机可以通过这样的公式直接读取这个内存地址中储存的数据，不需要任何额外的操作，一发命中，所以其时间复杂度为 $\\mathbf{O(1)}$ ","date":"2021-03-02","objectID":"/data-structures-array/:2:1","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"访问越界 由于访问数组需要指定下标，那么当指定的下标不存在时，会发生读取错误，称为访问越界。这个错误在不同的语言中，有不同的表现。 如果是 Java 这种语言层做了越界检查的语言，就会抛出 java.lang.ArrayIndexOutOfBoundsException，但是如果是 C 语言的话，可能会发生不可预知的错误，比如无限循环。所以访问数组时，需要警惕访问越界错误。 ","date":"2021-03-02","objectID":"/data-structures-array/:2:2","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"查找 有些地方说数组适合查找，查找的时间复杂度是 $O(1)$，这其实是不对的。 随机读取的前提是知道元素的下标，随机读取并不是查找，查找是要在数组中找到某个目标元素，在找到之前是不知道下标的，所以查找的时间复杂度不是 $\\mathbf{O(1)}$。 查找有不同的实现算法，即便是排好序的数组，用二分法查找，时间复杂度也是 $O(log_n)$ ","date":"2021-03-02","objectID":"/data-structures-array/:2:3","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"插入 如果我们要在一个长度为 10，且前 5 个位置都已经有数据的数组中，插入一个新的元素到第 3 个位置，时间复杂度是多少？ 例如现有数组 [bily,tony,jack,oliver,james]，我们要将新同学 harry 插入到 jack 现在的位置上，这里需要分两种情况来看 ","date":"2021-03-02","objectID":"/data-structures-array/:3:0","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"有序插入 如果插入之后需要保持元素的顺序，插入后的数组应该是这样 [bily,tony,harry,jack,oliver,james]，那么第 3 位及以后的所有元素都需要往后挪一位。 这种情况下，最好时间复杂度是 $O(1)$，就是插入的元素刚好在最后一位的情况，不需要移动任何元素； 最坏时间复杂度是 $O(n)$，就是插入的元素在第一位的情况，所有元素都需要移动一次； 那么平均时间复杂度是多少呢？因为插入到每个位置的概率都是一样的，所以平均时间复杂度是 $\\frac{1+2+\\cdots+n}{n}=\\mathbf{O(n)}$ ","date":"2021-03-02","objectID":"/data-structures-array/:3:1","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"无顺插入 如果插入之后不需要保持元素的顺序，那么插入时直接交换 jack 和 harry 的位置即可：[bily,tony,harry,oliver,james,jack] 此时无论将新元素插入到哪个位置上，都只需要操作新旧两个元素，因此时间复杂度是 $\\mathbf{O(1)}$ ","date":"2021-03-02","objectID":"/data-structures-array/:3:2","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"扩容 因为数组使用的是一块连续的内存，所以在使用之前就需要提前向计算机申请内存，也就是在声明数组时，需要确定数组的大小。 当数组中的空间耗尽时，就无法插入新元素，此时需要手动给数组进行“扩容”，也就是新申请一个更大的数组，然后将原来的数据搬过去，再插入新数据。这是一个比较耗时的操作，时间复杂度是 $\\mathbf{O(n)}$ ","date":"2021-03-02","objectID":"/data-structures-array/:3:3","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"删除 ","date":"2021-03-02","objectID":"/data-structures-array/:4:0","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"立即删除 还是因为数组使用连续内存空间的特征，所以在删除了其中一个元素后，这个位置不能空着，否则内存空间就不连续了，那么就需要把后面的数据都往前挪一位。 这个操作和保持顺序插入是比较类似的，如果删除的刚好是最后一个元素，那时间复杂度就是 $O(n)$，如果删除的是第一个元素，那时间复杂度就是 $O(n)$，而平均时间复杂度是 $\\mathbf{O(n)}$ ","date":"2021-03-02","objectID":"/data-structures-array/:4:1","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"延后删除 除了踏踏实实的立即删除外，为了优化数组删除操作的性能，工程师们有一些巧思妙想。 例如，我们要在数组 [bily,tony,harry,jack,oliver,james] 中依次删除 bily 和tony，如果操作一次就删除一次，那么就会造成两次数据搬运，造成性能损耗。 实际上，在大部分情况下，删除就是为了让被删除的数据无法被读取，那么我们只需要将这两个数据标记为已删除，而不是真正的删除，让其无法被读取，然后等到数据空间不够的时候，再一次性删除所有被标记为已删除的数据，只需要进行一次数据搬运操作。 在操作频繁的情况下中，这样可以大大提高数组操作的性能表现。 这个先标记再延迟删除的思路，其实就是 Java 虚拟机中标记清除垃圾回收算法的核心思想。数据结构和算法的魅力就在于此，很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是可拓展可迁移，才是最有价值的。 ","date":"2021-03-02","objectID":"/data-structures-array/:4:2","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"总结 数组是一种适合多读少写的数据结构，随机访问效率很高，时间复杂度是 $O(1)$，但是插入和删除操作相对低效，时间复杂度是 $O(n)$ 数据本身是一种很基础的数据结构，提供的 API 比较有限，复杂操作并不方便。很多编程语言都提供了以数组为基础的容器，提供了更多的 API，使用起来更加方便，如果不是对性能有非常高的要求，日常开发中使用容器更方便。 基于数组本身的特点，在使用数据容器（例如 Java 中的 ArrayList）时，尽量提前设置大小，可以提高效率。 ","date":"2021-03-02","objectID":"/data-structures-array/:5:0","tags":["DataStructure"],"title":"数据结构之数组","uri":"/data-structures-array/"},{"categories":["Computer Science"],"content":"执行效率和资源消耗是评价一个算法优劣最核心的两个点，分别代表运行这个算法所需要消耗的时间和空间。 分析运行一个算法需要消耗多少时间的行为叫做时间复杂度分析，同理，分析运行一个算法需要消耗多少空间的行为叫做空间复杂度分析。 实际上，如果我们直接将算法运行一遍，记录下运行过程中消耗的时间和空间，就可以得到这个算法准确的资源消耗情况。这种思路确实是可以，我们可以管这种方法叫做事后统计法。 但是这样得出来的结果变量太多，比如测试环境的硬件性能，软件（操作系统 / SDK 等）版本，测试的数据规模，甚至是实现算法所用的编程语言，都对结果都有或多或少的影响。而如果想要测试条件覆盖尽可能多的情况，就会产生极其巨大的工作量，即便是一个很简单的算法，都需要非常大量的测试。这显然不是一个可取的方法。 因此，事后统计法一般只适合在特定环境下给特定的程序做性能测试（例如很多程序在上线之前都要做压力测试），不适合用来对一个通用型的算法或程序做无差别的性能评估。 ","date":"2021-02-06","objectID":"/introduction-to-algorithm-complexity-analysis/:0:0","tags":["Algorithm"],"title":"算法复杂度分析入门","uri":"/introduction-to-algorithm-complexity-analysis/"},{"categories":["Computer Science"],"content":"大 O 表示法 为了在程序运行前，不依赖具体测试结果，就能无差别的评估一个算法的执行效率，工程师们使用了一个数学中的概念：大 O 符号，又称渐进符号 大 O 表示法是目前做算法复杂度分析时所使用的行业标准，这是一个抽象的概念，它表示的不是某个具体的值，而是一种趋势。 比如下面是一个计算 1 + 2 + 3 + … + n 的累加式的函数： int foo(int n) { int sum = 0; for(int i = 1; i \u003c= n; i++) { sum = sum + i; } return sum; } 假定计算机每执行一行代码所消耗的时间为 $t$，那么在这个例子中，第二行是一个简单的赋值操作，消耗时间为 $t$，第四、五行是一个循环操作，消耗时间为 $2n*t$，那么程序总消耗时间为 $T(n)= t + 2nt$ 再看下面这个例子： int bar(int n) { int sum = 0; for(int i = 1; i \u003c= n; i++) { for(int j = 1; j \u003c= n; j++) { sum = sum + i * j; } } return sum; } 在这个例子中，第二行耗费的时间为 $t$，三、四、五行是一个双层循环，其中第四、五行耗费的时间是 $2n^2t$，第三行耗费的时间是 $nt$，那么总的时间消耗为 $T(n)=t+nt+2n^2t$ 通过两个例子，可以发现一个明显的规律：代码的总执行时间 $T(n)$ 与每行代码的执行次数 $n$ 成正比。 利用大 O 符号，我们可以把这个规律表示为：$$T(n) = O(f(n))$$ 这个表达式称之为 大 O 表达式，其中：$T(n)$ 表示程序总的执行时间，$f(n)$ 表示每行代码执行的总次数，$n$ 表示数据规模的大小，$O$ 表示 $T(n)$ 与 $f(n)$ 成正比。 所以，大 O 表达式实际上并不是代表程序具体执行的时间，而是表示代码执行时间随数据规模增长的变化趋势，因此，也叫做渐进时间复杂度（asymptotic time complexity），简称时间复杂度。 在大 O 表达式中，当 $n$ 趋近于无限大时，公式中的常量、系数、低阶部分对增长趋势的影响非常小，可以忽略不计，因为现代计算机的执行速度非常快，通常能达到每秒几百亿次，因此我们只需要记录一个最大量级即可。 因此，第一个例子中我们得出的 $T(n)=t+2nt$，忽略常量 $t$ 和系数 $2t$ 之后，用大 O 表示法为：$T(n)=O(n)$ 第二个例子中的 $T(n)=t+nt+2n^2t$ 忽略常量和系数，保留最大量级后，用大 O 表示法为：$T(n)=O(n^2)$ （这个例子在后文“加法法则”中会有更详细的解释） 这种分析思路来源于数学中的 渐进分析 方法，计算机科学实际上就是从数学中独立出来的一个分支，在解决比较底层和抽象层面的问题上，用的基本都是数学方法。 ","date":"2021-02-06","objectID":"/introduction-to-algorithm-complexity-analysis/:1:0","tags":["Algorithm"],"title":"算法复杂度分析入门","uri":"/introduction-to-algorithm-complexity-analysis/"},{"categories":["Computer Science"],"content":"时间复杂度分析 时间，是世间最宝贵的资源。 在算法的复杂度分析中，时间复杂度分析通常比空间复杂度分析更重要，也更复杂。因为计算机的空间（硬件）是可以扩容的，而人类目前的科技能力，还远远达不到“扩容时间”的水平。 因此，我们先重点关注时间复杂度分析。 ","date":"2021-02-06","objectID":"/introduction-to-algorithm-complexity-analysis/:2:0","tags":["Algorithm"],"title":"算法复杂度分析入门","uri":"/introduction-to-algorithm-complexity-analysis/"},{"categories":["Computer Science"],"content":"分析技巧 分析代码的时间复杂度一般都需要按照具体逻辑具体分析，不过也有一些可以帮助我们简化分析的技巧，下面分享两个最常用的分析技巧。 乘法法则 如下示例： int foo(int n) { int sum = 0; for(int i = 1; i \u003c= n; i++) { sum = sum + bar(i); } return sum; } int bar(int n) { int sum = 0; for(int i = 1; i \u003c= n; i++) { sum = sum + i; } return sum; } 这其实是一个拆成两个函数的嵌套循环，如果函数 foo() 的循环体中是一个简单的操作，那么两个函数的时间复杂度都是 $O(n)$，但是函数 foo() 的循环体中调用了函数 bar()，而函数bar()的时间复杂度是$O(n)$，因此函数 foo() 的时间复杂度就是 $O(n)*O(n) = O(n^2)$ 如上所示，复杂度分析的乘法法则抽象为数学公式就是： 当碰到嵌套循环代码时，可以使用乘法法则帮助我们分析复杂度：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积。 例如外层循环时间复杂度为 $O(f(n))$，内层循环时间复杂度为 $O(g(n))$，那么总的时间复杂度就是 $O(f(n)*g(n))$ 加法法则 如下示例： int cal(int n) { int sum = 0; for(int i = 1; i \u003c= 100; i++) { sum = sum + i; } sum = sum + foo(n); sum = sum + bar(n); return sum; } int foo(int n) { int sum = 0; for(int i = 1; i \u003c= n; i++) { sum = sum + i; } return sum; } int bar(int n) { int sum = 0; for(int i = 1; i \u003c= n; i++) { sum = sum + foo(n); } return sum; } 根据上面的计算规则，我们知道函数 foo() 的时间复杂度 $T_1(n) = O(n)$，函数 bar() 的时间复杂度 $T_2(n) = O(n^2)$，而函数 cal() 中的第一段是个 100 次的循环，虽然是个循环，但是个已知次数的常量循环，与数据规模 $n$ 无关。 同时，随着数据规模的增大，$O(n^2)$ 的时间复杂度会远高于 $O(n)$，此时只需要取复杂度最高的值即可，因此总复杂度 如上所示，复杂度分析的加法法则抽象为数据公式就是： 当碰到程序中包含多个子函数/算法时，可以使用加法法则帮助我们分析复杂度：总复杂度等于量级最大的那段代码的复杂度。 技巧 如果在多段代码中，无法确定哪一段复杂度最大，那么直接相加即可。 如 $T_1=O(n)$，$T_2=O(m)$，在无法确定 $m$ 和 $n$ 谁更大时，则 $T(n)=O(n+m)$ 大部分情况下，复杂的算法或者程序都是由简单的算法步骤组合而成，这两个技巧有助于我们在分析复杂算法的时候理清思路，不被绕晕。理解这两个法则之后会发现，都是很自然的规律，所以不用刻意去记忆和套用，多分析几个实例，自然就熟悉了，而且可以总结出更适合自己思维习惯的技巧。 ","date":"2021-02-06","objectID":"/introduction-to-algorithm-complexity-analysis/:2:1","tags":["Algorithm"],"title":"算法复杂度分析入门","uri":"/introduction-to-algorithm-complexity-analysis/"},{"categories":["Computer Science"],"content":"分析实例 虽然代码千变万化，但是得益于数学思想的高度抽象性，以下几种形式就能囊括绝大部分算法的复杂度（按时间复杂度量级升序排列）： 常数阶：$O(1)$ 对数阶：$O(log\\ n)$ 线性阶：$O(n)$ 线性对数阶：$O(n*log\\ n)$ k次方阶：$O(n^k)$ 指数阶：$O(k^n)$ 阶乘阶：$O(n!)$ 以上 7 种量级的复杂度，可以分为两类：多项式量级和非多项式量级，其中非多项式量级只有两个：$O(k^n)$ 和 $O(n!)$。 时间复杂度为非多项式量级的算法问题也叫做 NP（Non-Deterministic Polynomial 非确定多项式）问题 随着数据规模的增大，非多项式时间复杂度量级的算法，执行时间会急剧增加，所以，非多项式时间复杂度的算法一般效率都很低，除非是确定性的小规模数据应用场景，大多是情况下都不应该使用，否则容易造成程序性能急剧下降。 我们主要看下几种常见的多项式时间复杂度的例子： 常数阶 首先还是要重申一下大 O 表达式的概念：表示程序执行时间随数据规模增长的变化趋势。 所以 $O(1)$ 并不是说只执行一行代码，而是指执行时间不会随着数据规模的增大而无增大，比如下面这个例子： int sum() { int i = 1; int j = 2; return i + j; } 即便需要执行 3 行代码，它的时间复杂度也是 $O(1)$，而不是 $O(3)$ 通常情况下，如果一段代码里没有循环、递归，其时间复杂度都是 $O(1)$，其函数图形如下 线性阶 线性阶也是一种非常常见的时间复杂度量级，比如文章一开始我们就分析过的第一个例子： int foo(int n) { int sum = 0; for(int i = 1; i \u003c= n; i++) { sum = sum + i; } return sum; } 这就是一个典型的 $O(n)$ 复杂度的例子，程序执行时间的增长幅度和数据规模的增长幅度保持一致。 通常情况下，一个简单的单层循环的时间复杂度就是 $O(n)$ ，其函数图形如下： 对数阶 对数阶是也是很常见，但是分析起来复杂一些的时间复杂度类型。看下面这个例子： int foo(int n) { int i = 1; while (i \u003c n) { i = i * 2; } return i; } // 这段代码的逻辑是： i 在循环中不断乘 2 ，直到 i \u003e= n 时，退出循环 根据上文的分析，我们只要分析出这个循环执行了多少次，就能算出这段代码的时间复杂度。 实际上就是等比数列：，也就是 $2^x = n$ 求解 $x$，高中数学给了我们答案：$x = log_2n$ ，所以这个循环执行了 $log_2n$ 遍，因此这段代码的时间复杂度为 $O(log_2n)$ 如果我们把上述例子中的常量乘数 2 换成 3，那么时间复杂度就变成了 $O(log_3n)$ 根据对数的链式运算，我们知道 $log_3n = log_32 * log_2n$，那么 $O(log_3n) = O(log_32 * log_2n)$，其中 $log_32$ 是一个常量系数，基于上文我们的分析，在大 O 表示法中，可以忽略系数，即： $O(C*f(n)) = O(f(n))$ 也就是说，在 $O(log_kn)$ 这样的对数阶复杂度中，无论底数常量 $k$ 是多少，复杂度都是一样的，所以我们统一忽略底数，将对数阶复杂度表示为：$O(log\\ n)$，其函数图像图下： 线性对数阶 理解了对数阶 $O(log\\ n)$ 之后，理解线性对数阶就容易很多了。将一个 $O(log\\ n)$ 复杂度的步骤，执行 $n$ 遍，其复杂度就是 $O(nlog\\ n)$ 如下是一个简单的 $O(nlog_n)$ 时间复杂度程序的示例： int foo(int n) { for(int j = 1; j \u003c= n; j++) { int i = 1; while (i \u003c= n) { i = i * 2; } } } // 这段代码中，外面套的这层 for 循环并没有什么实际意义，仅作为演示说明 根据前文我们提到的乘法法则，这段代码的时间复杂度很容易计算得出： $T(n) = O(n) * O(log_n) = O(nlog_n)$ 这就是线性对数阶，其函数图像如下： K 次方阶 这也是最常见的时间复杂度之一，$K$ 是常数，如下是一个非常简单的平方阶时间复杂度示例： int foo(int n) { int sum = 0; for(int i = 1; i \u003c= n; i++) { for(int j = 1; j \u003c= n; j++) { sum = sum + i * j; } } } 利用上文介绍的乘法法则，很快就能计算出这段代码的时间复杂度： $T(n)=O(n)*O(n)=O(n^2)$ 其函数图像图下： ⚠️ 注意：次方阶 $O(n^k)$ 容易和指数阶 $O(k^n)$ 混淆，它们都是指数函数，区别在于数据规模 $n$ 是底数还是指数，如果数据规模 $n$ 是指数的话，复杂度会急剧上升，参考前文的 NP 问题 ","date":"2021-02-06","objectID":"/introduction-to-algorithm-complexity-analysis/:2:2","tags":["Algorithm"],"title":"算法复杂度分析入门","uri":"/introduction-to-algorithm-complexity-analysis/"},{"categories":["Computer Science"],"content":"分析进阶 以上都是一些逻辑相对简单的例子，我们在实际编写程序的时候，通常会有一些提升效率的操作和技巧，这会让我们在分析时间复杂的过程中要考虑的因素更多一些。 如下示例： // array 是一个数组 int find(int[] array, int x) { for (int i = 0; i \u003c array.length -1 ; i++) { if (array[i] == x) { return i; } } return -1; } // 这个函数的逻辑是找出数组 array 中目标数值 x 的下标位置，如果数组中没有目标值 x ，就返回 -1 这段程序是在遍历数组，但又不完全是在遍历数组。因为循环中间的这个 return 导致我们不得不思考一个问题：如果 x 就在数组的第一位，那么这个函数就并不会遍历整个数组，而是在第一次循环就退出，这个时候时间复杂度就是 $O(1)$，但是如果 x 在数组中的最后一位，或是数组中根本不存在 x，那么就需要遍历整个数组，这个时候时间复杂度就变成了 $O(n)$ （$n$ 是数组的长度） 所以这段代码在不同情况下，时间复杂度是不一样的。 为了表示不同情况下的时间复杂度，我们需要再引入三个概念：最好时间复杂度，最坏时间复杂度和平均时间复杂度。 最好/最坏时间复杂度 最好和最坏时间复杂度比较简单，顾名思义，它们分别表示在最理想和最不理想的情况下代码的时间复杂度。 对应上述的例子，就是 x 在数组第一位，和 x 在数组最后一位或者不在数组中的情况。 平均时间复杂度 很明显，最理想和最不理想的情况发生的概率都很低，这样评估复杂度的话，不够准确。为了更准确的评估这种情况下的时间复杂度，我们就需要另一个概念：平均时间复杂度。 还是上面那个例子，x 在数组中位置的可能性有 $n+1$ 种情况，分别是在数组中的任意位置和不在数组中。我们把每种情况下需要执行的次数加起来，再除以 $n+1$ 就可以得到平均情况下的时间复杂度：$$\\frac{1+2+3+\\cdots+n+n}{n+1}$$ 我们把这个式子稍微简化一下，我们知道 $1+2+3+\\cdots+n=\\frac{n(n+1)}{2}$（推导过程可以看这里）所以：$$\\frac{1+2+\\cdots n+n}{n+1}=\\frac{n(n+1)/2+n}{n+1}=\\frac{n(n+3)}{2(n+1)}$$ 忽略常量和系数之后可以看出，这是个线性阶的时间复杂度（看不出来的话带入几个数算算也行），也就是 $O(n)$，这个就是平均时间复杂度。 但是这样计算平均时间复杂度，还是不够准确，因为每种情况出现的概率并不一样。首先从大方向上看， X 只能是在或不在数组中，这两种情况准确的概率统计起来并不容易。为了方便理解，我们将其概率分别假设为 1/2 ，同时，x 在数组中不同位置出现的概率也分别为 1/n，所以，根据概率乘法法则，x 出现在数组中各个不同位置的概率为：$$\\frac{1}{2}\\star\\frac{1}{n}=\\frac{1}{2n}$$ 然后，我们再将各种情况的概率因素考虑进去，平均时间复杂度的计算就变成了：$$1*\\frac{1}{2n}+2*\\frac{1}{2n}+\\cdots+n*\\frac{1}{2n}+n*\\frac{1}{2}=\\frac{3n+1}{4}$$ 这个值就是概率论中的加权平均值，也叫做期望值，所以更准确的平均时间复杂度，应该叫做加权平均时间复杂度，或者期望时间复杂度。 $\\frac{3n+1}{4}$去掉常量和系数之后，用大O表达式描述，加上概率之后的平均复杂度还是 $O(n)$，有时简单平均和加权平均复杂度是一样的，也有时会不一样。 平均时间复杂度的计算方式确实有些复杂，但是平时使用得其实并不多，多数情况下使用单一复杂度评估即可。 均摊时间复杂度 上文提到，只有在一些特殊情况下才需要使用平均时间复杂度去评估程序，而均摊时间复杂度更加特殊，使用到的情况更少。如下代码： // array表示一个长度为n的数组 // 代码中的array.length就等于n int[] array = new int[n]; int count = 0; void insert(int val) { if (count == array.length) { int sum = 0; for (int i = 0; i \u003c array.length; ++i) { sum = sum + array[i]; } array[0] = sum; count = 1; } array[count] = val; ++count; } 这段代码实现了一个往数组中插入数据的功能。当数组满了之后，也就是代码中的 count == array.length 时，我们用 for 循环遍历数组求和，并清空数组，将求和之后的 sum 值放到数组的第一个位置，然后再将新的数据插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。 我们分析下这段代码的最好、最坏和平均时间复杂度。 最好的情况就是数组刚好有空闲，于是直接插入即可，时间复杂度是 $O(1)$，最坏的情况就是刚好 count == array.length ，这个时候需要遍历整个数组，时间复杂度是 $O(n)$ 然后我们再来分析一下期望时间复杂度，这里一共有 n+1 种情况，其中有空位的种情况有 n 种的，没有空位的情况有一种，而且每种情况出现的概率都是 n+1，因此，期望时间复杂度为： $$1*\\frac{1}{n+1}+1*\\frac{1}{n+1}+\\cdots+1*\\frac{1}{n+1}+n*\\frac{1}{n+1}=\\frac{2n}{n+1}$$ 省略系数和常量之后，就是 $O(1)$ 但是这个例子中分析平均时间复杂度，其实并不需要这么复杂的计算，为什呢？ 我们对比一下这两个例子，会发现两个很大的区别： find() 函数中，只有在极少数最理想的情况下，时间复杂度才会是 $O(1)$，而大部分情况下时间复杂度都是 $O(n)$，但是 insert() 函数很不一样，在大部分情况下，时间复杂度都是 $O(1)$，只有在极少数的最不理想情况下，时间复杂度才会是 $O(n)$ insert() 函数中 $O(n)$ 操作和 $O(1)$ 操作有很强的规律性和顺序性：每一次 $O(n)$ 的插入操作之后，紧接着 $n-1$ 次的 $O(1)$ 插入操作，如此循环 针对这种更加特殊的情况，我们还有一种更加方法的分析方法：摊还分析法，通过这种方法分析得出的时间复杂度我们称之为：摊还时间复杂度。 这种分析方法的核心思路是 分组。 我们看这个例子，每一次 $O(n)$ 复杂度的插入操作之后，紧接着 $n-1$ 次 $O(1)$ 复杂度的插入操作，那么我们把这 n 次操作分成一组，将第一次 $O(n)$ 耗时多的操作中消耗的时间，均摊到后面 $n-1$ 耗时少的操作中，这样一组连续操作下来均摊的时间复杂度就是 $O(1)$。这就是摊还分析方法的大致思路。 总结一下就是，在一个程序中，大部分时候时间复杂度都很低，只有极少数时候时间复杂度比较高，而且这些操作之间存在很强的顺序关系，这个时候就可以把这个程序的操作进行分组，将耗时多的情况下多消耗的时间，均摊到耗时少的情况中去，这样就能更快的得到这个程序的时间复杂度。 从这个例子中，我们也能看出来，在能够使用摊还分析方法的场景中，一般均摊时间复杂度就等于最好时间复杂度。 通过上述平均时间复杂度和均摊时间复杂度的两个例子，我们能看出来，平均时间复杂度是分析程序时间复杂度的一种特殊情况，而均摊时间复杂度又是平均时间复杂度的一种特殊情况，因此，会用到均摊时间复杂度的场合就更少了。所以，我们重点是要知道这种分析思路，并不用花太多精力在去区分和记忆。 上文花了很长的篇幅讲了时间复杂度分析，理解了这些内容，下面我们再来分析空间复杂度，就简单很多了。 ","date":"2021-02-06","objectID":"/introduction-to-algorithm-complexity-analysis/:2:3","tags":["Algorithm"],"title":"算法复杂度分析入门","uri":"/introduction-to-algorithm-complexity-analysis/"},{"categories":["Computer Science"],"content":"空间复杂度分析 分析空间复杂度和分析时间复杂度的框架是一致的，都是使用大 O 表达式，也都是表示数据规模与复杂度之间的趋势关系。那么，类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法消耗的存储空间与数据规模之间的增长关系。 空间复杂度分析相较于时间复杂度分析，要简单很多。看下面这个例子： void print(int n) { int[] array = new int[n]; for(int i = 0; i \u003c= n-1; i++) { array[i] = (int)(Math.random()*100); } for(int j = 0; j \u003c= n-1; j++) { System.out.println(array[j]); } } 这段代码是将一个长度为 n 的数组用 100 以内的随机数填满，然后再遍历输出。 在空间使用方面，除了第二行申请了一个长度为 n 的数组 array，其他操作均没有涉及，因此这段代码的空间复杂度和 n 的大小成正比，也就是 $O(n)$ 常用的空间复杂度比常用的时间复杂度也少很多，一般就是 $O(1)$、 $O(n)$、 $O(n^2)$ 这三种。如果碰到特殊情况，使用分析时间复杂度的思路去分析即可。 本文主要参考《数据结构与算法之美》和维基百科。 ","date":"2021-02-06","objectID":"/introduction-to-algorithm-complexity-analysis/:3:0","tags":["Algorithm"],"title":"算法复杂度分析入门","uri":"/introduction-to-algorithm-complexity-analysis/"},{"categories":null,"content":"写清楚，讲明白。 ","date":"2021-01-01","objectID":"/about/:0:1","tags":null,"title":"About","uri":"/about/"},{"categories":["Database"],"content":"最近项目上需要用到 redis 高可用方案，遂上网找了一些资料学习，但是网上关于 redis 高可用的几种实现方式或口径不一，或含糊不清，或缺斤少两。经历了多方资料学习和实际验证，本文试图将 redis 分布式和集群方案从概念理解到技术选型到搭建使的整个过程用最简单的语言讲述清楚。 ","date":"2020-10-20","objectID":"/the-ultimate-guide-to-redis-distributed-high-availability/:0:0","tags":["Redis","Distributed"],"title":"Redis 分布式高可用终极指南","uri":"/the-ultimate-guide-to-redis-distributed-high-availability/"},{"categories":["Database"],"content":"分布式与集群 下文会涉及到大量的分布式和集群术语，这里我们先来复习一下集群和分布式的概念，加深一下理解。 目前的项目很少会采用单机架构了，一是因为单机性能有限，二是因为单机服务一旦故障整个系统就无法继续提供服务了。所以目前集群和分布式的架构使用得很广泛，主要就是为了解决上述两个问题，一个性能问题，一个故障问题，通过分布式架构解决性能（高并发）问题，通过集群架构解决故障服务（高可用）问题。 ","date":"2020-10-20","objectID":"/the-ultimate-guide-to-redis-distributed-high-availability/:1:0","tags":["Redis","Distributed"],"title":"Redis 分布式高可用终极指南","uri":"/the-ultimate-guide-to-redis-distributed-high-availability/"},{"categories":["Database"],"content":"分布式架构 分布式：一个业务分拆多个子业务，部署在不同的服务器上 网上很多文章把分布式架构说得很复杂，但都没有切中关键，其实理解起来很简单，所有的计算机系统都是为业务服务的，将同一个业务拆分成多个子业务，各个子业务部署在不同的服务器上，这就是分布式架构，通过将业务拆细，为不同的子业务配置不同性能的服务器，提高整个系统的性能。我个人认为目前很火的微服务概念其实本质上就是分布式。 按照类型大致可以分为两种：分布式计算和分布式存储。 分布式计算很好理解，就是将大量计算任务分配到多个计算单元上以提高总计算性能。例如暴力破解某个密码需要遍历某个字符组合10万次，假设一台计算机需要10分钟，那么10台计算机同时遍历，每台遍历1万次，最后将结果汇总，那么就只需要1分钟。这10台计算机组合起来就是一个分布式计算系统，这里的业务就是计算。 同理，分布式储存也很好理解，就是将大量数据分配到多个储存单元上以提高总存储量。例如100ZB的数据一个储存单元放不下，那就拆成100份，每个储存单元存1份，那么这100个存储单元组合起来就是一个分布式储存系统，这里的业务就是存储。目前主流的关系型数据库都有比较成熟的分布式存储方案，如 MySQL 的 MySQL Fabric、MyCat 等，Oracle Database 有Oracle Sharding 等。Redis 作为流行的非关系型数据库，由于是内存数据库，理论上一般不会在 Redis 中存放太多的数据，但是在某些特殊情况下还是会有储存空间不够的情况，或者需要预防储存空间不够的情况发生，这个时候就需要 Redis 分布式架构了。 例如某集团下有很多的子公司，每个子公司都有多套 IT 系统，其中很多 IT 系统都是需要使用 Redis 的，集团为了统一管理，搭建了一套中央 Redis 系统，要求所有子公司下的 IT 系统统一使用集团的中央 Redis 库，这个时候即使当前储存容量够用，但是为了应对后期发展就必须使用到分布式储存，因为分布式架构理论上都支持无限水平拓展。 ","date":"2020-10-20","objectID":"/the-ultimate-guide-to-redis-distributed-high-availability/:1:1","tags":["Redis","Distributed"],"title":"Redis 分布式高可用终极指南","uri":"/the-ultimate-guide-to-redis-distributed-high-availability/"},{"categories":["Database"],"content":"集群架构 集群：同一个业务，部署在多个服务器上 集群同样也非常好理解，就是在多个服务器上部署同一个业务，这样可以起到两个作用： 分散每台服务器的压力 任意一台或者几台服务器宕机也不会影响整个系统 例如一个典型的 Web 集群服务架构图如下： 这里三个 Web Server 服务器实际上都是运行着同一套业务，但是三台服务器就可以显著分散单台服务器压力，并且任意一台宕机也不会导致无法提供服务。 ","date":"2020-10-20","objectID":"/the-ultimate-guide-to-redis-distributed-high-availability/:1:2","tags":["Redis","Distributed"],"title":"Redis 分布式高可用终极指南","uri":"/the-ultimate-guide-to-redis-distributed-high-availability/"},{"categories":["Database"],"content":"分布式与集群的关系 分布式和集群区别很好理解，用下面一张图表示： 需要注意的是分布式不一定能用上，但是集群一般都是需要的。因为不是所有系统都需要应对高并发场景，但高可用是一个系统能够长期稳定运行基本保障。因此用到分布式架构的系统基本上都会用到集群，而用集群架构的系统却不一定会用到分布式。 ","date":"2020-10-20","objectID":"/the-ultimate-guide-to-redis-distributed-high-availability/:1:3","tags":["Redis","Distributed"],"title":"Redis 分布式高可用终极指南","uri":"/the-ultimate-guide-to-redis-distributed-high-availability/"},{"categories":["Database"],"content":"Redis 部署指南 ","date":"2020-10-20","objectID":"/the-ultimate-guide-to-redis-distributed-high-availability/:2:0","tags":["Redis","Distributed"],"title":"Redis 分布式高可用终极指南","uri":"/the-ultimate-guide-to-redis-distributed-high-availability/"},{"categories":["Database"],"content":"单节点方案：Redis Standalone 原理简介 这是最简单的 redis 部署方案，所有数据存储和读写操作都在同一个 redis 服务上。 这种方式优点很明显：部署简单，无论是部署成本还是运维成本都很低，本地测试时最常见也是最方便的方式。 但同时缺点也很明显：不能实现高可用，也不能应对高并发场景，也无法轻易水平拓展，数据储存量很容易见顶。 部署实例 单节点模式的部署是最简单的，一下是 Linux 系统下部署单节点 redis 的方法： # 下载 Redis 二进制安装包： wget http://download.redis.io/releases/redis-5.0.4.tar.gz # 解压二进制包 tar –zxvf redis-5.0.4.tar.gz # 进入解压文件夹并编译二进制文件 cd redis-5.0.4 make # 安装 cd src make test make install 在 make install 和 make test 的时候可能会遇到下面这个问题： You need tcl 8.5 or newer in order to run the Redis test make: *** [test] Error 1 这是因为系统中的 TCL 语言版本太低，TCL 语言是一种工具脚本语言，在安装 Redis 的过程中 make test 命令需要用到这个脚本语言，这个时候我们需要升级一下系统中的 TCL 版本： # 下载一个高于 8.5 版本的 TCL 安装包，比如 8.6.8 wget http://downloads.sourceforge.net/tcl/tcl8.6.8-src.tar.gz # 解压 tar -zxvf tcl8.6.8-src.tar.gz -C /usr/local/ # 切换到解压后的源码目录 cd /usr/local/tcl8.6.8/unix/ # 编译和安装 sudo ./configure sudo make sudo make install 升级 TCL 到 8.5 版本以后，继续执行之前报错的语句，完成 Redis 的安装，安装完成后用 redis-server -v 验证安装是否成功，若成功输出如下版本信息则代表安装成功： Redis server v=5.0.4 安装成功就可以直接运行了，但是默认配置下是不支持后台运行的，观点命令窗口就会结束 redis 进程，这显然是不行的。所以我们再简单改一下 redis 的配置，让其能够直接后台运行。 # 进入到 redis 的安装目录，编辑 redis.conf vim /usr/redis/redis-5.0.4/redis.conf # 将 daemonize no 修改成 daemonize yes （使 redis 服务可以在后台运行） # 在指定配置下运行redis服务 /usr/local/bin/redis-server /usr/redis/redis-5.0.4/redis.conf # 查看redis运行情况 ps -ef | grep redis # 输出 app 21794 1 0 Jan28 ? 03:31:25 ./redis-server *:6379 可以看到 redis 在默认的 6379 端口下运行，配置文件中还有一些可以调整的地方，这里就不一一列举了。 那么单节点模式 redis 服务就部署完成了 ","date":"2020-10-20","objectID":"/the-ultimate-guide-to-redis-distributed-high-availability/:2:1","tags":["Redis","Distributed"],"title":"Redis 分布式高可用终极指南","uri":"/the-ultimate-guide-to-redis-distributed-high-availability/"},{"categories":["Database"],"content":"Redis 高可用方案：Redis Sentinel 原理简介 Redis Sentinel 是 Redis 官方推荐的高可用性(HA)解决方案，这是生产环境中最实用也是最常用的方案。 这里涉及到另一个概念：master-slaver（主从模式）。很好理解，就是常用的主备模式，例如 nginx 的主备模式。一个主 redis 节点可以配置多个从节点，当主节点挂掉时，从节点自动顶上代替主节点，这样就可以有效的避免一个节点挂掉导致整个系统挂掉的问题，实现 redis 服务的高可用。如下图： 但是这个方案需要解决两个基本问题： 如何提前判断各个节点（尤其是主节点）的运行健康状况？ 当主节点宕机的时候如何从多个从节点中选出一个作为新的主节点并实现自动切换？ 这时 Redis Sentinel 应运而生，它主要有以下三个特点： 监控（Monitoring）：Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification）：当被监控的某个 Redis 服务器出现问题时，Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）：当一个主服务器不能正常工作时，Sentinel 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器，并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时， 集群也会向客户端返回新主服务器的地址，使得集群可以使用新主服务器代替失效服务器。 总结来说就是 sentinel 可以监控一个或者多个 master-slaver 集群，定时对每个节点进行健康检查，可以通过 API 发送通知，并自动进行故障转移。这时r redis 结构就变成了 使用了 redis sentinel 之后客户端不再直接连接 redis 节点获取服务，而是使用 sentinel 代理获取 redis 服务，类似 Nginx 的代理模式。那么这里又有一个新问题，就是如果 sentinel 宕机了，那么客户端就找不到 redis 服务了，所以 sentinel 本身也是需要支持高可用。 **好在sentinel 本身也支持集群部署，并且各个 sentinel 之间支持自动监控，如此一来 redis 主从服务和 sentinel 服务都可以支持高可用。**预期结构如下： 部署实例 master-slaver 一主二从 那么下面我们就来实操一下，以下过程大部分参考 redis 官方 Redis Sentinel 文档。 安装 redis 就不重复了，和单机 redis 一样。 redis 解压后，redis home 目录下有 redis 配置的样例文件，我们不直接在此文件上就行修改，在redis home目录下新建文件夹 master-slave ，将配置文件都放于此目录下，下面是三个 redis 节点配置的关键部分 master 配置文件：redis-6379.conf port 6379 daemonize yes logfile \"6379.log\" dbfilename \"dump-6379.rdb\" dir \"/opt/soft/redis/data\" slave-1 配置文件：redis-6380.conf port 6380 daemonize yes logfile \"6380.log\" dbfilename \"dump-6380.rdb\" dir \"/opt/soft/redis/data\" # 关键配置：将这个 redis 指定为某个第一个 redis 的 slaver slaveof 127.0.0.1 6379 slave-2 配置文件：redis-6381.conf port 6381 daemonize yes logfile \"6381.log\" dbfilename \"dump-6381.rdb\" dir \"/opt/soft/redis/data\" # 关键配置：将这个 redis 指定为某个第一个 redis 的 slaver slaveof 127.0.0.1 6379 分别启动这三个 redis 服务，启动过程就不罗嗦了，和分别启动三个单机 redis 是一样的，分别指定三个配置文件即可。启动后如下图所示： 6379、6380、6381 端口分别在运行一个 redis-server。 接下来查看这三个 redis-server 之间的关系：连接到主 redis 上用 info replication即可查看 可以看到当前连接的 redis 服务为 master 角色，下面有两个 slaver，IP 和端口都能看到。 这样我们就顺利的完成了 一主二从 redis 环境的搭建，下面开始搭建 sentinel 集群。 sentinel 集群 sentinel 本质上是一个特殊的 redis，大部分配置和普通的 redis 没有什么区别，主要区别在于端口和其哨兵监控设置，下面是三个典型的 sentinel 配置文件中的关键内容： sentinel-26379.conf #设置 sentinel 工作端口 port 26379 #后台运行 daemonize yes #日志文件名称 logfile \"26379.log\" #设置当前 sentinel 监控的 redis ip 和 端口 sentinel monitor mymaster 127.0.0.1 6379 2 #设置判断 redis 节点宕机时间 sentinel down-after-milliseconds mymaster 60000 #设置自动故障转移超时 sentinel failover-timeout mymaster 180000 #设置同时故障转移个数 sentinel parallel-syncs mymaster 1 sentinel-26380.conf #设置 sentinel 工作端口 port 26380 #后台运行 daemonize yes #日志文件名称 logfile \"26380.log\" #设置当前 sentinel 监控的 redis ip 和 端口 sentinel monitor mymaster 127.0.0.1 6379 2 #设置判断 redis 节点宕机时间 sentinel down-after-milliseconds mymaster 60000 #设置自动故障转移超时 sentinel failover-timeout mymaster 180000 #设置同时故障转移个数 sentinel parallel-syncs mymaster 1 sentinel-26381.conf #设置 sentinel 工作端口 port 26391 #后台运行 daemonize yes #日志文件名称 logfile \"26381.log\" #设置当前 sentinel 监控的 redis ip 和 端口 sentinel monitor mymaster 127.0.0.1 6379 2 #设置判断 redis 节点宕机时间 sentinel down-after-milliseconds mymaster 60000 #设置自动故障转移超时 sentinel failover-timeout mymaster 180000 #设置同时故障转移个数 sentinel parallel-syncs mymaster 1 针对几个监控设置的配置做一下详细说明： sentinel monitor [master-group-name] [ip] [port] [quorum] 这个命令中【master-group-name】是 master redis 的名称；【ip】和【port】分别是其 ip 和端口，很好理解。最后一个参数【quorum】是”投票数“ 举个栗子，redis 集群中有3个 sentinel 实例，其中 master 挂掉了，如果这里的票数是2，表示有2个 sentinel 认为 master 挂掉啦，才能被认为是正真的挂掉啦。其中 sentinel 集群中各个 sentinel 之间通过 gossip 协议互相通信。 具体怎样投票还涉及到 redis 集群中的【主观下线】和【客观下线】的概念，后面再详细介绍。 down-after-milliseconds sentinel 会向 master 发送心跳 PING 来确认 master 是否存活，如果 master 在“一定时间范围”内不回应PONG 或者是回复了一个错误消息，那么这个 sentinel 会主观地认为这个 master 已经不可用了。而这个down-after-milliseconds 就是用来指定这个“一定时间范围”的，单位是毫秒。 failover-timeout 这个参数 redis 官方文档中并未做详细说明，但是很好理解，就是 sentinel 对 redis 节点进行自动故障转移的超时设置，当 failover（故障转移）开始后，在此时间内仍然没有触发任何 failover 操作，当前sentinel 将会认为此次故障转移失败。 parallel-syncs 当新master产生时，同时进行 slaveof 到新 master 并进行同步复制的 slave 个数,也就是同时几个 slave 进行同步。因为在 salve 执行 salveof 与新 master 同步时，将","date":"2020-10-20","objectID":"/the-ultimate-guide-to-redis-distributed-high-availability/:2:2","tags":["Redis","Distributed"],"title":"Redis 分布式高可用终极指南","uri":"/the-ultimate-guide-to-redis-distributed-high-availability/"},{"categories":["Database"],"content":"Redis 分布式高可用方案：Redis Cluster 原理简介 作为一个内存数据库，实现高可用是一个基本保障，当储存服务在可预见的将来需要做存储拓展时，分布式储存就是一个必须要考虑到的事情。例如部署一个中央 redis 储存服务，提供给集团下所有的子公司所有需要的系统使用，并且系统数量在不断的增加，此时在部署服务的时候，分布式储存结构几乎是必然的选择。 Redis 3.0 版本之前，可以通过前面说所的 Redis Sentinel（哨兵）来实现高可用 ( HA )，从 3.0 版本之后，官方推出了Redis Cluster，它的主要用途是实现数据分片(Data Sharding)，同时提供了完整的 sharding、replication（复制机制仍使用原有机制，并且具备感知主备的能力）、failover 解决方案，称为 Redis Cluster，同样可以实现 HA，是官方当前推荐的方案。 在 Redis Sentinel 模式中，每个节点需要保存全量数据，冗余比较多，而在Redis Cluster 模式中，每个分片只需要保存一部分的数据，对于内存数据库来说，还是要尽量的减少冗余。在数据量太大的情况下，故障恢复需要较长时间，另外，内存的价格也是非常高昂的。 Redis Cluste r的具体实现细节是采用了 Hash 槽的概念，集群会预先分配16384个槽（slot），并将这些槽分配给具体的服务节点，通过对 Key 进行 CRC16(key)%16384 运算得到对应的槽是哪一个，从而将读写操作转发到该槽所对应的服务节点。当有新的节点加入或者移除的时候，再来迁移这些槽以及其对应的数据。在这种设计之下，我们就可以很方便的进行动态扩容或缩容。 当然，关于高可用的实现方案，也可以将 Redis-Sentinel 和 Redis-Cluster 两种模式结合起来使用，不过比较复杂，并不太推荐。 下图展示了 Redis Cluster 分配 key 和 slot 的基本原理： 一个典型的 Redis Cluster 分布式集群由多个Redis节点组成。不同的节点组服务的数据无交集，每个节点对应数据 sharding 的一个分片。节点组内部分为主备 2 类，对应前面叙述的 master 和 slave。两者数据准实时一致，通过异步化的主备复制机制保证。一个节点组有且仅有一个 master，同时有0到多个 slave。只有 master 对外提供写服务，读服务可由 master/slave 提供。如下所示： 上图中，key-value 全集被分成了 5 份，5个 slot（实际上Redis Cluster有 16384 [0-16383] 个slot，每个节点服务一段区间的slot，这里面仅仅举例）。A和B为master节点，对外提供写服务。分别负责 1/2/3 和 4/5 的slot。A/A1 和B/B1/B2 之间通过主备复制的方式同步数据。 上述的5个节点，两两通过 Redis Cluster Bus 交互，相互交换如下的信息： 1、数据分片（slot）和节点的对应关系； 2、集群中每个节点可用状态； 3、集群结构发生变更时，通过一定的协议对配置信息达成一致。数据分片的迁移、主备切换、单点 master 的发现和其发生主备关系变更等，都会导致集群结构变化。 4、publish/subscribe（发布订阅）功能，在Cluster版内部实现所需要交互的信息。 Redis Cluster Bus 通过单独的端口进行连接，由于Bus是节点间的内部通信机制，交互的是字节序列化信息。相对 Client 的字符序列化来说，效率较高。 Redis Cluster是一个去中心化的分布式实现方案，客户端和集群中任一节点连接，然后通过后面的交互流程，逐渐的得到全局的数据分片映射关系。 更多更详细的 redis cluster 的说明请移步Redis Cluster 官方文档 部署实例 Redis Cluster 集群至少需要三个 master 节点，本文将以单机多实例的方式部署3个主节点及3个从节点，6个节点实例分别使用不同的端口及工作目录 安装 redis 同上，不赘述。 为每个 redis 节点分别创建工作目录 在redis安装目录 /usr/local/redis-5.0.2 下新建目录 redis-cluster，并在该目录下再新建6个子目录，7000,7001,8000,8001,9000,9001，此时目录结构如下图所示： 修改配置 #开启后台运行 daemonize yes #工作端口 port 7000 #绑定机器的内网IP或者公网IP,一定要设置，不要用 127.0.0.1 bind 172.27.0.8 #指定工作目录，rdb,aof持久化文件将会放在该目录下，不同实例一定要配置不同的工作目录 dir /usr/local/redis-cluster/7000/ #启用集群模式 cluster-enabled yes #生成的集群配置文件名称，集群搭建成功后会自动生成，在工作目录下 cluster-config-file nodes-7000.conf #节点宕机发现时间，可以理解为主节点宕机后从节点升级为主节点时间 cluster-node-timeout 5000 #开启AOF模式 appendonly yes #pid file所在目录 pidfile /var/run/redis_8001.pid 按照上面的样例将配置文件复制到另外5个目录下，并对 port、dir、cluster-config-file 三个属性做对应修改，这里就不一一列举了。 安装 Ruby 和 RubyGems 由于创建 redis cluster 需要用到 redis-trib 命令，而这个命令依赖 Ruby 和 RubyGems，因此需要安装一下。 [root@VM_0_15_centos redis-cluster]# yum install ruby [root@VM_0_15_centos redis-cluster]# yum install rubygems [root@VM_0_15_centos redis-cluster]# gem install redis --version 3.3.3 分别启动6个节点 [root@VM_0_15_centos redis-4.0.6]# ./src/redis-server redis-cluster/7000/redis.conf [root@VM_0_15_centos redis-4.0.6]# ./src/redis-server redis-cluster/7001/redis.conf [root@VM_0_15_centos redis-4.0.6]# ./src/redis-server redis-cluster/8000/redis.conf [root@VM_0_15_centos redis-4.0.6]# ./src/redis-server redis-cluster/8001/redis.conf [root@VM_0_15_centos redis-4.0.6]# ./src/redis-server redis-cluster/9000/redis.conf [root@VM_0_15_centos redis-4.0.6]# ./src/redis-server redis-cluster/9001/redis.conf 查看服务运行状态 [root@VM_0_15_centos redis-4.0.6]# ps -ef | grep redis root 20290 1 0 18:33 ? 00:00:02 ./src/redis-server *:8001 [cluster] root 20295 1 0 18:33 ? 00:00:02 ./src/redis-server *:8002 [cluster] root 20300 1 0 18:33 ? 00:00:02 ./src/redis-server *:8003 [cluster] root 20305 1 0 18:33 ? 00:00:02 ./src/redis-server *:8004 [cluster] root 20310 1 0 18:33 ? 00:00:02 ./src/redis-server *:8005 [cluster] root 20312 1 0 18:33 ? 00:00:02 ./src/redis-server *:8006 [cluster] root 22913 15679 0 19:31 pts/2 00:00:00 grep --color=auto redis 可以看到6个节点以及全部成功启动 创建 redis cluster [root@VM_0_15_centos redis-4.0.6]# ./src/redis-trib.rb create --replicas 1 172.27.0.8:7000 172.27.0.8:7001 172.27.0.8:8000 172.27.0.8:8001 172.27.0.8:9000 172.27.0.8:90001 创建过程中会有部分需要确认的地方，按照提示输入即可，集群创建完毕后观察一下这个集群的节点状态 172.27.0.8:7000\u003e cluster nodes 068ac2afe1ade8b69b8322","date":"2020-10-20","objectID":"/the-ultimate-guide-to-redis-distributed-high-availability/:2:3","tags":["Redis","Distributed"],"title":"Redis 分布式高可用终极指南","uri":"/the-ultimate-guide-to-redis-distributed-high-availability/"},{"categories":["Database"],"content":"总结 Redis 服务的部署方案的选型大家根据自己项目的需求部署即可，一般来说 redis sentinel 就够用了，也是目前用得最多的模式，但是 redis 3.0 之后官方推出的 redis-cluster 虽然本质是用于实现数据分片和分布式存储，但是其也实现了 redis sentinel 的全部功能，有完全的 HA 能力，并且部署起来更简单，因此成为了官方推荐的 HA 方案。我个人也更加推荐 redis cluster 方案。 ","date":"2020-10-20","objectID":"/the-ultimate-guide-to-redis-distributed-high-availability/:2:4","tags":["Redis","Distributed"],"title":"Redis 分布式高可用终极指南","uri":"/the-ultimate-guide-to-redis-distributed-high-availability/"}]